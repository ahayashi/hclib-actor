{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"HClib-Actor Documentation","text":""},{"location":"#overview","title":"Overview","text":"<p>This document describes an actor/selector-based C/C++ programming systems for distributed systems.</p>"},{"location":"#papers","title":"Papers","text":""},{"location":"#the-programming-model-and-runtime-system","title":"The Programming Model and Runtime System","text":"<ul> <li> <p>A Productive and Scalable Actor-based Programming System for PGAS Applications. Sri Raj Paul, Akihiro Hayashi, Kun Chen, and Vivek Sarkar. 22th International Conference on Computational Science (ICCS2022), June 2022. [doi, pdf]</p> </li> <li> <p>A Fine-grained Asynchronous Bulk Synchronous parallelism model for PGAS applications. Sri Raj Paul, Akihiro Hayashi, Kun Chen, Youssef Elmougy, Vivek Sarkar. Journal of Computational Science, April 2023. [doi] When citing our programming model and runtime system, please cite this paper.</p> </li> <li> <p>Towards Safe HPC: Productivity and Performance via Rust interfaces for a Distributed C++ Actors library. John T. Parrish, Nicole Wren, Tsz Hang Kiang, Akihiro Hayashi, Jeffrey Young, Vivek Sarkar. 20th International Conference on Managed Programming Languages &amp; Runtimes (MPLR, co-located with SPLASH). October 2023. [doi]</p> </li> <li> <p>Enabling User-level Asynchronous Tasking in the FA-BSP Model - Case Study: Distributed Triangle Counting. Akihiro Hayashi, Shubhendra Pal Singhal, Youssef Elmougy, Jiawei Yang. The Vivek Sarkar Festschrift Symposium (VIVEKFEST2024, co-located with SPLASH24). October 2024. [doi]</p> </li> <li> <p>Enhancing Productivity and Performance of HClib-Actor with Efficient Task Termination. Youssef Elmougy, Nirjhar Deb, Akihiro Hayashi, Vivek Sarkar. 27th Workshop on Advances in Parallel and Distributed Computational Models (APDCM, co-located with IPDPS25) [doi].</p> </li> </ul>"},{"location":"#applications","title":"Applications","text":"<ul> <li> <p>Highly Scalable Large-Scale Asynchronous Graph Processing using Actors. Youssef Elmougy, Akihiro Hayashi, Vivek Sarkar. IEEE TCSC International Scalable Computing Challenge (SCALE 2023, co-located with CCGRID23), May 2023. Recipient of Best SCALE Challenge Award. [doi, video]</p> </li> <li> <p>A Distributed, Asynchronous Algorithm for Large-Scale Internet Network Topology Analysis. Youssef Elmougy, Akihiro Hayashi, Vivek Sarkar. IEEE TCSC International Scalable Computing Challenge (SCALE 2024, co-located with CCGRID24), May 2024. Recipient of Best SCALE Challenge Award. [doi]</p> </li> <li> <p>Asynchronous Distributed Actor-based Approach to Jaccard Similarity for Genome Comparisons. Youssef Elmougy, Akihiro Hayashi, Vivek Sarkar. International Conference on High Performance Computing (ISC High Performance). May 2024. [doi]</p> </li> <li> <p>Asynchronous Distributed-Memory Parallel Algorithms for Influence Maximization. Shubhendra Pal Singhal, Souvadra Hati, Jeffrey Young, Vivek Sarkar, Akihiro Hayashi, Richard Vuduc. International Conference for High Performance Computing, Networking, Storage, and Analysis (SC24). Nov 2024. [doi]</p> </li> <li> <p>ActorISx: Exploiting Asynchrony for Scalable High-Performance Integer Sort. Youssef Elmougy, Shubhendra Pal Singhal, Akihiro Hayashi, Vivek Sarkar. IEEE TCSC International Scalable Computing Challenge (SCALE 2025, co-located with CCGRID25). May 2025 [doi]</p> </li> <li> <p>Asynchronous Distributed-Memory Parallel Algorithm for k-mer Counting. Souvadra Hati, Akihiro Hayashi, Richard Vuduc. 39th IEEE International Parallel &amp; Distributed Processing Symposium (IPDPS25). June 2025. [doi]</p> </li> <li> <p>Divide, Conquer, and Match: A Distributed and Asynchronous Approach for Subgraph Isomorphism. Youssef Elmougy, Akihiro Hayashi, Vivek Sarkar. Workshop on Graphs, Architectures, Programming, and Learning (GrAPL, co-located with IPDPS25) June 2025. [doi]</p> </li> </ul>"},{"location":"#performance-evaluationstools","title":"Performance Evaluations/Tools","text":"<ul> <li> <p>On the Cloud We Can\u2019t Wait: Asynchronous Actors Perform Even Better on the Cloud. Aniruddha Mysore, Youssef Elmougy, Akihiro Hayashi. The Vivek Sarkar Festschrift Symposium (VIVEKFEST2024, co-located with SPLASH24). October 2024. [doi]</p> </li> <li> <p>ActorProf: A Framework for Profiling and Visualizing Fine-grained Asynchronous Bulk Synchronous Parallel Execution. Jiawei Yang, Shubhendra Pal Singhal, Jun Shirako, Akihiro Hayashi, Vivek Sarkar. SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis(SC24-Workshop). Nov 2024. [doi]</p> </li> </ul>"},{"location":"#posters","title":"Posters","text":"<ul> <li> <p>Genome Assembly Using an Asynchronous Distributed Actor-Based Approach. Souvadra Hati, Richard Vuduc. International Conference for High Performance Computing, Networking, Storage, and Analysis (SC23, Poster Session, ACM-SRC Finalist). November 2023. [link]</p> </li> <li> <p>Accelerating Actor-Based Distributed Triangle Counting. Aniruddha Mysore, Kaushik Ravichandran, Youssef Elmougy, Akihiro Hayashi, Vivek Sarkar. International Conference for High Performance Computing, Networking, Storage, and Analysis (SC23, Poster Session). November 2023. [link]</p> </li> <li> <p>Bottleneck Scenarios in use of the Conveyors Message Aggregation Library. Shubhendra Pal Singhal, Akihiro Hayashi, Vivek Sarkar. IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS24, Poster Session). May 2024. [doi]</p> </li> </ul>"},{"location":"#links","title":"Links","text":"<ul> <li>Github Repository</li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>If you have any questions about HClib-Actor, please contact us at: habanero@cc.gatech.edu</p>"},{"location":"contributors/","title":"Contributors","text":""},{"location":"contributors/#hclib-actor-contributors","title":"HClib-Actor Contributors","text":"<ul> <li>Sri Raj Paul</li> <li>Akihiro Hayashi</li> <li>Youssef Elmougy</li> <li>Shubhendra Pal Singhal</li> <li>Jiawei Yang</li> <li>Vivek Sarkar</li> </ul>"},{"location":"api/reference/","title":"API Reference","text":""},{"location":"api/reference/#hclibs-actor","title":"<code>hclib</code>'s Actor","text":"API Description <code>launch()</code> Create an hclib context. <code>finish()</code> Wait until 1) all outgoing messages are sent, and 2) all incoming messages are processed on the current PE. <code>send()</code> Send an asynchronous message to a remote PE. <code>done()</code> Tell the runtime that the current PE will not send any messages to a specific mailbox. <code>yield()</code> Explicitly occur a context-switching to make progress in communication."},{"location":"background/","title":"Background","text":""},{"location":"background/actor/","title":"Actor Model","text":""},{"location":"background/actor/#what-is-the-actor-model","title":"What is the Actor model?","text":"<p>The actor model is a message-based concurrency model. In the world of the actor model, you can imagine that multiple actors are concurrently running and communicating each other via message-passing. </p> <p> </p> The Actor Model <p>An actor has 1) an immutable identity, 2) encapsulated mutable local state , 3) procedures to manipulate the local state, and 4) logical thread of control. An actor may 1) send messages to other actors, 2) process received messages one-by-one, 3) change local state, and 4) even create new actors.</p>"},{"location":"background/actor/#what-are-typical-api-routines-in-the-actor-model","title":"What are typical API routines in the Actor model?","text":"<ul> <li><code>new</code>: create an actor.</li> <li><code>start()</code>: let the actor start sending and receiving messages.</li> <li><code>exit()</code>: terminate the actor.</li> <li><code>send()</code>: send a message to another actor.</li> <li><code>process()</code>: process a received message from another actor.</li> </ul>"},{"location":"background/actor/#reference","title":"Reference","text":"<ul> <li> <p>Paul, Sri Raj, Akihiro Hayashi, Kun Chen, and Vivek Sarkar. \"A Productive and Scalable Actor-Based Programming System for PGAS Applications.\" In International Conference on Computational Science, pp. 233-247. Springer, Cham, 2022.</p> </li> <li> <p>\"COMP 322: Fundamentals of Parallel Programming Lecture 21: Actors\" by Prof. Vivek Sarkar.</p> </li> </ul>"},{"location":"background/bale/","title":"Summary","text":""},{"location":"background/bale/#what-is-bale","title":"What is Bale?","text":"<p>Bale provides a library-based distributed programming model with a special focus on optimizing fine-grained communications. </p> <p><code>hclib-actor</code> mainly depends on the following libraries from Bale:</p> <ul> <li><code>libgetput</code>: a library that enables one-sided communications, remote atomics, and collectives built on top of UPC/OpenSHMEM</li> <li><code>spmat</code>: a library that facilitates the construction/manipulation of distributed sparse matrices, which is built on top of <code>libgetput</code></li> <li><code>conveyors</code>: a communication aggregation library, built on top of <code>libgetput</code></li> <li><code>exstack/exstack2</code>: a preliminary experimental version of <code>conveyors</code>.  </li> </ul> <p>For more details, please see the github repo for Bale can be found here.</p>"},{"location":"background/bale/#how-bale-is-used-in-hclib-actor","title":"How Bale is used in <code>hclib-actor</code>?","text":"<p>The key motivation for introducing <code>hclib-actor</code> is to simplify programming with <code>conveyors</code> while keeping the same performance. Specifically, note that <code>conveyors</code> requires ninja-level distributed programming skills as illustrated in this Histogram with conveyors example and providing a higher-level programming model would benefit non-ninja programmers such as domain experts and scientists.</p> <p><code>hclib-actor</code> is mainly designed to provide an actor based distributed programming model and completely abstracts away <code>conveyors</code>. Additionally, many of kernels and benchmarks in <code>hclib-actor</code> use routines from <code>spmat</code> and <code>libgetput</code> libraries. </p> <p>For more details, please see the followings:</p> <ul> <li>spmat</li> <li>libgetput</li> </ul> <p>Note</p> <p>It is worth noting that <code>hclib-actor</code> does not necessarily depend on <code>spmat</code> or <code>libgetput</code> libraries. For example, for PGAS SPMD execution, bare OpenSHMEM and UPC can be used instead of <code>libgetput</code>. </p>"},{"location":"background/bale/#further-readings","title":"Further Readings","text":"<ul> <li>Jason DeVinney and John Gilbert. 2022. Bale: Kernels for irregular parallel computation (Not a benchmark) https://github.com/jdevinney/bale/blob/master/docs/Bale-StGirons-Final.pdf</li> <li>F. Miller Maley and Jason G. DeVinney. 2019. Conveyors for Many-to-Many Streaming Communication https://github.com/jdevinney/bale/blob/master/docs/uconvey.pdf</li> </ul>"},{"location":"background/bsp/","title":"Bulk Synchronous Parallel","text":""},{"location":"background/bsp/#what-is-the-bulk-synchronous-parallel-model","title":"What is the bulk synchronous parallel model?","text":"<p>The Bulk Synchronous Parallel (BSP) model is one of the most popular parallel computation models. </p> <p>The model consists of:</p> <ul> <li>A set of processor-memory pairs.</li> <li>A communication network that delivers messages in a point-to-point manner.</li> <li>Efficient barrier synchronization for all or a subset of the processes.</li> </ul> <p> </p> The BSP Model <p>The horizontal structure of the BSP model indicates a set of computation can be distributed across a fixed number of virtual processors.</p> <p>The vertical structure can be viewed as a sequence of \"supersteps\" separated by barriers, in which each processor performs local computation and (a)synchronous communications in a superstep, and the role of the barrier is to ensure that all communications in a superstep have been completed before moving to the next superstep.</p>"},{"location":"background/bsp/#single-program-multiple-data-spmd-programming","title":"Single Program Multiple Data (SPMD) Programming","text":"<p>One realization of the BSP model on the software side is the SPMD-style programming. In SPMD, virtual processors execute the same program independently, which is a good fit not only for realizing the BSP model, but also for writing a distributed program for large-scale systems in a scalable way. </p> <p>The example code below shows a basic structure of a BSP program written by the SPMD-style (OpenSHMEM/MPI):</p> OpenSHMEMMPI <pre><code>#include &lt;shmem.h&gt;\n// the main function is executed by multiple PEs\nint main(void) {\n  shmem_init();\n  int npes = shmem_n_pes(); // get the number of the PEs\n  int mype = shmem_my_pe(); // get my PE ID\n  // superstep\n  {\n    ...  // local computation\n    ...  // communication\n    shmem_barrier(); // barrier\n  }\n  ...\n  shmem_finalize();\n}\n</code></pre> <pre><code>#include &lt;mpi.h&gt;\n// the main function is executed by multiple PEs\nint main(void) {\n  MPI_Init(NULL, NULL);\n  int myRank, nRanks;\n  MPI_Comm_size(MPI_COMM_WORLD, &amp;nRanks); // get the number of the ranks\n  MPI_Comm_rank(MPI_COMM_WORLD, &amp;myRank); // get my rank ID\n  // superstep\n  {\n    ...  // local computation\n    ...  // communication\n    MPI_Barrier(); // barrier\n  }\n  ...\n  MPI_Finalize();\n}\n</code></pre>"},{"location":"background/bsp/#further-readings","title":"Further Readings","text":"<ul> <li>Leslie G. Valiant. 1990. A bridging model for parallel computation. Commun. ACM 33, 8 (Aug. 1990), 103\u2013111. https://doi.org/10.1145/79173.79181</li> </ul>"},{"location":"background/hclib/","title":"Habanero-C Library (HClib)","text":""},{"location":"background/hclib/#what-is-hclib","title":"What is HClib?","text":"<p>Habanero C/C++ library (HClib) is a lightweight asynchronous many-task (AMT) programming model-based runtime. It uses a lightweight work-stealing scheduler to schedule the tasks. HClib uses a persistent thread pool called workers, on which tasks are scheduled and load balanced using lock-free concurrent deques. HClib exposes several programming constructs to the user, which in turn helps them to express parallelism easily and efficiently.</p> <p>A brief summary of the relevant APIs is as follows:</p> <ul> <li><code>launch</code>: Used for creating an HClib context.</li> <li><code>async</code>: Used for creating asynchronous tasks dynamically.</li> <li><code>finish</code>: Used for bulk task synchronization. It waits on all tasks spawned (including nested tasks) within the scope of the finish.</li> <li><code>promise</code> and <code>future</code>: Used for point-to-point inter-task synchronization in C++11. A promise is a single-assignment thread-safe container, that is used to write some value and a future is a read-only handle for its value. Waiting on a future causes a task to suspend until the corresponding promise is satisfied by putting some value to the promise.</li> </ul>"},{"location":"background/hclib/#an-example-hclib-program","title":"An example HClib program","text":"<p>The following example creates an HClib context in which there is a <code>finish</code> scope that waits on a task created by <code>async</code>. Since the task assign 1 to <code>ran</code>, after the <code>finish</code> scope, the value of <code>ran</code> should be 1.</p> <pre><code>#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;assert.h&gt;\n\n#include \"hclib_cpp.h\"\n\nint ran = 0;\n\nint main (int argc, char ** argv) {\n    const char *deps[] = { \"system\" };\n    hclib::launch(deps, 1, []() {\n        hclib::finish([]() {\n            printf(\"Hello\\n\");\n            hclib::async([&amp;](){ ran = 1; });\n        });\n    });\n    assert(ran == 1);\n    printf(\"Exiting...\\n\");\n    return 0;\n}\n</code></pre>"},{"location":"background/hclib/#further-readings","title":"Further Readings","text":"<ul> <li>M. Grossman, V. Kumar, N. Vrvilo, Z. Budimlic and V. Sarkar, \"A pluggable framework for composable HPC scheduling libraries,\" 2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW), 2017, pp. 723-732, doi:  https://doi.org/10.1109/IPDPSW.2017.13</li> </ul>"},{"location":"background/libgetput/","title":"libgetput","text":""},{"location":"background/libgetput/#libgetput-library","title":"<code>libgetput</code> library","text":"<p><code>libgetput</code> helps the user to write PGAS SPMD programs by offering different types of communication routines such as one-sided PUT/GET, remote atomics, barriers, and collectives. The library has two backends (UPC and OpenSHMEM) and the user can chose a backend when building bale.</p> <p>Many of <code>libgetput</code> routines are influenced by UPC. For example, <code>THREADS</code> refers to the number of PEs (= <code>shmem_n_pes()</code>) and <code>MYTHREAD</code> refers to the current PE number (= <code>shmem_my_pe()</code>). Also, <code>lgp_all_alloc(N, sizeof(T))</code> distributes N elements across all the PEs in a cyclic way, which is the same as <code>upc_all_alloc(N, sizeof(T))</code>. This would require SHMEM programmers to carefully use the routine because <code>shmem_malloc(N*sizeof(T))</code> allocates N elements per PE. A list of <code>libgetput</code> routines can be found here.</p>"},{"location":"background/libgetput/#further-readings","title":"Further Readings","text":"<ul> <li>README.md for libgetput https://github.com/jdevinney/bale/blob/master/src/bale_classic/libgetput/README.md</li> </ul>"},{"location":"background/openshmem/","title":"OpenSHMEM","text":""},{"location":"background/openshmem/#what-is-openshmem","title":"What is OpenSHMEM?","text":"<p>OpenSHMEM is a library-based PGAS programming model. OpenSHMEM is originally from SHMEM, which was firstly introduced by Cray back in 90s, and includes a specification for standardized SHMEM library routines. </p> <p>One of the interesting features of OpenSHMEM is symmetric variables. In an OpenSHMEM program, global variables and dynamically allocated variables (by <code>shmem_malloc</code>) exist with the same size, type, and relative address on all PEs, and can be directly fed into one-sided communication routines such as put/get, which significantly improves the programmability.</p> <p>The following is a working example that 1) allocates a symmetric integer variable on each PE, 2) put the value to the next PE, and 3) prints the value on each PE. Note that, like MPI, an OpenSHMEM program is written in an SPMD manner.</p> test.c<pre><code>#include &lt;shmem.h&gt;\n#include &lt;stdio.h&gt;\n// the main function is executed by multiple PEs\nint main(void) {\n  shmem_init();\n  int npes = shmem_n_pes(); // get the number of the PEs\n  int mype = shmem_my_pe(); // get my PE ID\n  // dynamic symmetric variable allocation\n  int* x = (int*)shmem_malloc(sizeof(int)); \n  // superstep\n  {\n    int val = mype; // local computation\n    // communication (put to next PE in a circular way)\n    shmem_p(x, val, (mype+1) % npes);\n    // barrier\n    shmem_barrier_all();\n  }\n  printf(\"x is %d on PE%d (out of %d PEs)\\n\", *x, mype, npes);\n  shmem_free(x);\n  shmem_finalize();\n}\n</code></pre> <p>An example output with 2PEs is as follows: <pre><code>oshcc test.c -o test\noshrun -n 2 ./test\nx is 1 on PE0 (out of 2 PEs)\nx is 0 on PE1 (out of 2 PEs)\n</code></pre></p>"},{"location":"background/openshmem/#further-readings","title":"Further Readings","text":"<ul> <li>Swaroop Pophale and Tony Curtis, 2011. OpenSHMEM tutorial. http://www.openshmem.org/site/sites/default/site_files/OpenSHMEM_PGAS11_tutorial.pdf</li> <li>OpenSHMEM Specification. http://www.openshmem.org/site/specification</li> <li>Barbara Chapman, Tony Curtis, Swaroop Pophale, Stephen Poole, Jeff Kuehn, Chuck Koelbel, and Lauren Smith. 2010. Introducing OpenSHMEM: SHMEM for the PGAS community. In Proceedings of the Fourth Conference on Partitioned Global Address Space Programming Model (PGAS '10). Association for Computing Machinery, New York, NY, USA, Article 2, 1\u20133. https://doi.org/10.1145/2020373.2020375</li> </ul>"},{"location":"background/pgas/","title":"Partitioned Global Address Space","text":""},{"location":"background/pgas/#what-is-partitioned-global-address-space-pgas","title":"What is Partitioned Global Address Space (PGAS)?","text":"<p>The PGAS (pronounced p-gas) is a programming model which gives the user the illusion of shared memory on distributed systems. As the name implies, you can imagine that there is a single shared global address space consisting of logical portions, each portion of which is local to each processing element (PE). Typically, many PGAS languages/libraries offer point-to-point one-sided communication, where the user can use 1) <code>put</code> routine to write data to a remote (non-local) location, 2) <code>get</code> routine to read data from a remote location, whereas MPI requires two-sided communication.</p> <p> </p> The PGAS Model <p>Note</p> <p>While recent versions of MPI supports one-sided communications, that is not as simple as what PGAS languages/libraries offer.</p>"},{"location":"background/pgas/#examples-of-pgas-languageslibraries","title":"Examples of PGAS languages/libraries","text":"<ul> <li>X10</li> <li>Chapel</li> <li>Unified Parallel C (UPC)</li> <li>Co-Array Fortran (CAF)</li> <li>OpenSHMEM</li> </ul>"},{"location":"background/spmat/","title":"spmat","text":""},{"location":"background/spmat/#spmat-library","title":"<code>spmat</code> library","text":"<p><code>spmat</code> library helps the user to construct and manipulate distributed sparse matrices. It uses a distributed version of the CSR format, in which the rows are distributed across all the PEs in a cyclic way.</p> <p>Suppose we have the following 4x4 matrix:</p> <pre><code>1 0 1 0\n0 1 0 1\n1 0 1 0\n0 0 1 1\n</code></pre> <p>If there are 2PEs, since <code>spmat</code> does a cyclic distribution, PE0 owns ROW0 and ROW2, PE1 owns ROW1 and ROW3. Note that the ROW numbers are global and it is convenient to use a local ROW number to access a local portion of the array on each PE. Specifically, a local row number can be computed by doing <code>GLOBAL_ROW / nPEs</code>. Also, the owner of a global row can be identified by computing <code>GLOBAL_ROW % nPEs</code>.</p> <pre><code>// 2PEs\nGLOBAL ROW 0, PE0's ROW0: 1 0 1 0\nGLOBAL ROW 1, PE1's ROW0: 0 1 0 1\nGLOBAL ROW 2, PE0's ROW1: 1 0 1 0\nGLOBAL ROW 3, PE1's ROW1: 0 0 1 1\n</code></pre> <p>A typical idiom for iterating over non-zeros in a specific local row <code>i</code> of a sparse matrix <code>A</code> is as follows:</p> <pre><code>for (int64_t j = A-&gt;loffset[i]; j &lt; A-&gt;loffset[i+1]; j++) {\n  // visit each non-zero\n  int64_t nonzero = A-&gt;lnonzero[j];\n}\n</code></pre> <p>Also, a typical idiom for iterating over remote non-zeros in a global row is as follows:</p> OpenSHMEMlibgetput <pre><code>int64_t global_row = 1;\nint64_t pe = global_row % shmem_n_pes(); // find the owner of \"global_row\"\nint64_t i = global_row / shmem_n_pes();  // compute the local row number\nint64_t start = shmem_int64_g(&amp;A-&gt;loffset[i], pe);\nint64_t end = shmem_int64_g(&amp;A-&gt;loffset[i+1], pe);\nfor(int j = start; j &lt; end; j++) {\n  int64_t nonzero = shmem_int64_g(&amp;A-&gt;lnonzero[j], pe);\n}\n</code></pre> <pre><code>int64_t global_row = 1;\nint64_t start = lgp_get_int64(A-&gt;offset, global_row);\nint64_t end = lgp_get_int64(A-&gt;offset, global_row+THREADS);\nfor(int j = start; j &lt; end; j++){\n  int64_t nonzero = lgp_get_int64(A-&gt;nonzero, j*THREADS + global_row%THREADS);\n}\n</code></pre> <p>It is worth noting that, unlike the typical CSR format, <code>nonzero</code> stores a global column number (0, 1, 2, 3 in our example, see below), which reduces memory size (c.f. the CSR format uses three arrays). For algorithms in which actual values need to be stored, bale3's  <code>spmat</code> allows the user to use <code>A-&gt;value[j]</code>. The type of <code>value[]</code> is double.</p> <pre><code>1 0 1 0  our representation  0 - 2 -\n0 1 0 1  -----------------&gt;  - 1 - 3\n1 0 1 0  -----------------&gt;  0 - 2 -\n0 0 1 1                      - - 2 3\n</code></pre> <p>Now let us use a working example that reads the following matrix market format file, which is equivalent to the matrix above, load it into a <code>sparsemat_t</code> object, and finally prints the contents of it on each PE:</p> test.mtx<pre><code>%%MatrixMarket matrix coordinate pattern\n4 4 8\n1 1\n1 3\n2 2\n2 4\n3 1\n3 3\n4 3\n4 4\n</code></pre> <p>Here is a OpenSHMEM/libget program that iterates over non-zeros on each PE and print it. Notice that the code uses all-to-all barrier (<code>shmem_barrier_all()</code> or <code>lgp_barrier()</code>) so PE0 first prints the contents, then PE1 does the printing:</p> OpenSHMEMlibgetput <pre><code>#include &lt;shmem.h&gt;\nextern \"C\" {\n  #include \"spmat.h\"\n}\n\n// SPMD\nint main(int argc, char * argv[]) {\n  shmem_init();\n\n  sparsemat_t *A;\n  A = read_matrix_mm_to_dist(\"./test.mtx\");\n\n  for (int pe = 0; pe &lt; shmem_n_pes(); pe++) {\n      if (pe != shmem_my_pe()) continue;\n      for (int64_t i = 0; i &lt; A-&gt;lnumrows; i++) {\n          for(int64_t j = A-&gt;loffset[i]; j &lt; A-&gt;loffset[i+1]; j++) {\n              printf(\"[PE%d] localrow:%d, lnonzero[%d]: %ld\\n\", pe, i, j, A-&gt;lnonzero[j]);\n          }\n      }\n      shmem_barrier_all();\n  }\n  shmem_barrier_all();\n  // PE0 gets global row 1\n  if (shmem_my_pe() == 0) {\n      int64_t global_row = 1;\n      int64_t pe = global_row % shmem_n_pes(); // find the owner of \"global_row\"\n      int64_t i = global_row / shmem_n_pes();  // compute the local row number\n      int64_t rowstart = shmem_int64_g(&amp;A-&gt;loffset[i], pe);\n      int64_t rowstart_next = shmem_int64_g(&amp;A-&gt;loffset[i+1], pe);\n      for(int j = rowstart; j &lt; rowstart_next; j++) {\n          printf(\"[PE0] non-zero at column %ld in PE%ld's local row %ld\\n\", shmem_int64_g(&amp;A-&gt;lnonzero[j], pe), pe, i);\n      }\n  }\n  shmem_barrier_all();\n  shmem_finalize();\n\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;shmem.h&gt;\nextern \"C\" {\n  #include \"spmat.h\"\n}\n\nint main(int argc, char * argv[]) {\n  lgp_init(argc, argv);\n\n  sparsemat_t *A;\n  A = read_matrix_mm_to_dist(\"./test.mtx\");\n\n  // iterating over local rows\n  for (int pe = 0; pe &lt; THREADS; pe++) {\n      if (pe != MYTHREAD) continue;\n      for (int i = 0; i &lt; A-&gt;lnumrows; i++) {\n          for(int j = A-&gt;loffset[i]; j &lt; A-&gt;loffset[i+1]; j++) {\n              printf(\"[PE%d] localrow:%d, lnonzero[%d]: %ld\\n\", pe, i, j, A-&gt;lnonzero[j]);\n          }\n      }\n      lgp_barrier();\n  }\n  lgp_barrier();\n\n  // PE0 gets global row 1\n  if (MYTHREAD == 0) {\n      int64_t global_row = 1;\n      int64_t pe = global_row % THREADS;\n      int64_t i = global_row / shmem_n_pes();\n      int64_t start = lgp_get_int64(A-&gt;offset, global_row);\n      int64_t end = lgp_get_int64(A-&gt;offset, global_row+THREADS);\n      for(int j = start; j &lt; end; j++){\n          printf(\"[PE0] non-zero at column %ld in PE%ld's local row %ld\\n\", lgp_get_int64(A-&gt;nonzero, j*THREADS + pe), pe, i);\n      }\n  }\n  lgp_barrier();\n  lgp_finalize();\n\n  return 0;\n}\n</code></pre> <p>Example output on 2PEs:</p> <pre><code>[PE0] localrow:0, lnonzero[0]: 0\n[PE0] localrow:0, lnonzero[1]: 2\n[PE0] localrow:1, lnonzero[2]: 0\n[PE0] localrow:1, lnonzero[3]: 2\n[PE1] localrow:0, lnonzero[0]: 1\n[PE1] localrow:0, lnonzero[1]: 3\n[PE1] localrow:1, lnonzero[2]: 2\n[PE1] localrow:1, lnonzero[3]: 3\n[PE0] non-zero at column 1 in PE1's local row 0\n[PE0] non-zero at column 3 in PE1's local row 0\n</code></pre> <p>You can also use <code>print_matrix(A)</code> from <code>spmat</code> library:</p> <pre><code>row 0: 0 2\nrow 1: 1 3\nrow 2: 0 2\nrow 3: 2 3\n</code></pre>"},{"location":"background/spmat/#further-readings","title":"Further Readings","text":"<ul> <li>README.md for spmat https://github.com/jdevinney/bale/blob/master/src/bale_classic/spmat/README.md</li> </ul>"},{"location":"details/basics/","title":"Step-by-step Guide","text":"<p>Our framework focuses on improving the performance and programmability of distributed applications where each PE sends/receives a massive number of fine-grain messages to/from random remote locations. Such applications are typically categorized as irregular applications such as graph analytics, sparse matrix linear algebra operations, and so on.</p> <p>The key features of our framework include:</p> <ul> <li>Asynchronous messaging with our actor/selector library (<code>hclib-actor</code>).</li> <li>SPMD-style programming with OpenSHMEM.</li> <li>Runtime automatic message aggregation backed by Bale. </li> </ul> <p>More details on SPMD, OpenSHMEM, Bale, Actor model can be found in the Background section.</p>"},{"location":"details/basics/#step-1-create-an-empty-hclib-actor-program","title":"Step 1: Create an empty hclib-actor program","text":"<p>To take a first step, let's first write an empty program that 1) initializes and finalizes SHMEM, and 2) launches an empty HClib program:</p> <pre><code>#include &lt;shmem.h&gt;\n#include \"selector.h\"\n// SPMD\nint main(int argc, char * argv[]) {\n\n  const char *deps[] = { \"system\", \"actor\" };\n  // Initialize SHMEM is done at the very beginning of hclib::launch()\n  // shmem_init();\n  hclib::launch(deps, 2, [=] {\n    // do nothing\n  });\n  // Finalize SHMEM is done at the very end of hclib::launch()\n  //shmem_finalize();  \n  return 0;\n}\n</code></pre> <p>As the names imply, <code>shmem_init()</code> initializes and <code>shmem_finalize()</code> finalizes SHMEM. Also, <code>hclib::launch</code> launches an HClib program expressed as a C++ lambda expression (<code>[=] {}</code>). The first two arguments indicate that it loads two plugins stored in the array <code>deps</code> (<code>libhclib_system.so</code> and <code>libhclib_actor.so</code>).</p>"},{"location":"details/basics/#step-2-allocate-memory-and-initialize-it","title":"Step 2: Allocate memory and initialize it","text":"<p>Then, let's allocate memory using <code>shmem_malloc()</code> and initialize it:</p> <pre><code>#include &lt;shmem.h&gt;\n#include \"selector.h\"\n\nvoid print_array(int *larray, const int N) {\n   for (int i = 0; i &lt; N; i++) {\n     printf(\"[PE%d] larray[%d] = %d\\n\", shmem_my_pe(), i, larray[i]);\n   }\n}\n\n// SPMD\nint main(int argc, char * argv[]) {\n  const char *deps[] = { \"system\", \"actor\" };\n  hclib::launch(deps, 2, [=] {\n    // allocate memory\n    const int N = 10;\n    int* larray = (int*)shmem_malloc(sizeof(int)*N);\n    for (int i = 0; i &lt; N; i++) {\n        larray[i] = N * shmem_my_pe() + i;\n    }\n    print_array(larray, N);\n    shmem_barrier_all();\n    shmem_free(larray);\n  });\n  return 0;\n}\n</code></pre> <p>Here each PE allocates an integer array with N elements and initializes it. Notice that now we have <code>print_array</code> function that prints the content of the array on the current PE. Here is an example output with 2PEs:</p> <pre><code>[PE0] larray[0] = 0\n[PE0] larray[1] = 1\n[PE0] larray[2] = 2\n[PE0] larray[3] = 3\n[PE0] larray[4] = 4\n[PE0] larray[5] = 5\n[PE0] larray[6] = 6\n[PE0] larray[7] = 7\n[PE0] larray[8] = 8\n[PE0] larray[9] = 9\n[PE1] larray[0] = 10\n[PE1] larray[1] = 11\n[PE1] larray[2] = 12\n[PE1] larray[3] = 13\n[PE1] larray[4] = 14\n[PE1] larray[5] = 15\n[PE1] larray[6] = 16\n[PE1] larray[7] = 17\n[PE1] larray[8] = 18\n[PE1] larray[9] = 19\n</code></pre> <p>Note</p> <p>While this example uses <code>shmem_malloc()</code> to create a globally accessible array, a regular <code>malloc()</code> can also be used. In that case, actor-based messaging is the only way to put/get data to/from remote PE's memory. We recommend using the standard <code>malloc()</code> function unless the allocated data is intended for use with SHMEM's communication routines.</p>"},{"location":"details/basics/#step-3-write-an-actor-program","title":"Step 3: Write an actor program","text":"<p>Now let us create an actor program in which each PE sends asynchronous messages that increment the content of <code>larray</code> by one on the receiver side. Like conventional actor programs, let us define an actor class with 1) a local state (<code>larray</code>) and 2) a message handler (<code>process()</code>):</p> <pre><code>class MyActor: public hclib::Selector&lt;1, int&gt; {\n    int *larray;\n    void process(int idx, int sender_rank) {\n        larray[idx] += 1;\n    }\npublic:\n    MyActor(int *larray) : larray(larray) {\n        mb[0].process = [this](int idx, int sender_rank) { this-&gt;process(idx, sender_rank);};\n    }\n};\n</code></pre> <p>Then, let us use the actor class from the main program:</p> <pre><code>MyActor* actor_ptr = new MyActor(larray);\nhclib::finish([=]() {\n    actor_ptr-&gt;start();\n    for (int i = 0; i &lt; N; i++) {\n        int pe = (shmem_my_pe() + 1) % shmem_n_pes();\n        actor_ptr-&gt;send(i, pe);\n    }\n    actor_ptr-&gt;done(0);\n});\n</code></pre> <p>In this example, each PE starts the actor class and sends <code>N</code> messages to the next PE (the last PE sends messages to PE0). (Add more)</p>"},{"location":"details/basics/#step-4-putting-it-altogether","title":"Step 4: Putting it altogether","text":"<p>Here is a final program:</p> <pre><code>#include &lt;shmem.h&gt;\n#include \"selector.h\"\n\nvoid print_array(int *larray, const int N) {\n   for (int i = 0; i &lt; N; i++) {\n     printf(\"[PE%d] larray[%d] = %d\\n\", shmem_my_pe(), i, larray[i]);\n   }\n}\n\nclass MyActor: public hclib::Selector&lt;1, int&gt; {\n    int *larray;\n    void process(int idx, int sender_rank) {\n        larray[idx] += 1;\n    }\npublic:\n    MyActor(int *larray) : larray(larray) {\n        mb[0].process = [this](int idx, int sender_rank) { this-&gt;process(idx, sender_rank);};\n    }\n};\n\n// SPMD\nint main(int argc, char * argv[]) {\n  const char *deps[] = { \"system\", \"actor\" };\n  hclib::launch(deps, 2, [=] {\n    // allocate memory\n    const int N = 10;\n    int* larray = (int*)shmem_malloc(sizeof(int)*N);\n    for (int i = 0; i &lt; N; i++) {\n      larray[i] = N * shmem_my_pe() + i;\n    }\n    print_array(larray, N);\n    MyActor* actor_ptr = new MyActor(larray);\n    hclib::finish([=]() {\n        actor_ptr-&gt;start();\n        for (int i = 0; i &lt; N; i++) {\n        int pe = (shmem_my_pe() + 1) % shmem_n_pes();\n        actor_ptr-&gt;send(i, pe);\n        }\n      actor_ptr-&gt;done(0);\n    });\n    shmem_barrier_all();\n    print_array(larray, N);\n    shmem_barrier_all();\n    shmem_free(larray);\n  });\n  return 0;\n}\n</code></pre> <pre><code>[PE0] larray[0] = 0\n[PE0] larray[1] = 1\n[PE0] larray[2] = 2\n[PE0] larray[3] = 3\n[PE0] larray[4] = 4\n[PE0] larray[5] = 5\n[PE0] larray[6] = 6\n[PE0] larray[7] = 7\n[PE0] larray[8] = 8\n[PE0] larray[9] = 9\n[PE1] larray[0] = 10\n[PE1] larray[1] = 11\n[PE1] larray[2] = 12\n[PE1] larray[3] = 13\n[PE1] larray[4] = 14\n[PE1] larray[5] = 15\n[PE1] larray[6] = 16\n[PE1] larray[7] = 17\n[PE1] larray[8] = 18\n[PE1] larray[9] = 19\n[PE0] larray[0] = 1\n[PE0] larray[1] = 2\n[PE0] larray[2] = 3\n[PE0] larray[3] = 4\n[PE0] larray[4] = 5\n[PE0] larray[5] = 6\n[PE0] larray[6] = 7\n[PE0] larray[7] = 8\n[PE0] larray[8] = 9\n[PE0] larray[9] = 10\n[PE1] larray[0] = 11\n[PE1] larray[1] = 12\n[PE1] larray[2] = 13\n[PE1] larray[3] = 14\n[PE1] larray[4] = 15\n[PE1] larray[5] = 16\n[PE1] larray[6] = 17\n[PE1] larray[7] = 18\n[PE1] larray[8] = 19\n[PE1] larray[9] = 20\n</code></pre>"},{"location":"details/fabsp/","title":"Fine-grained-Asynchronous Bulk-Synchronous Parallelism (FABS)","text":"The FA-BSP Model <p>The Fine-grained Asynchronous Bulk Synchronous Parallel (FA-BSP) model is an extended version of the BSP model that facilitates fine-grained asynchronous point-to-point messaging even during the local computation. As illustrated in the figure above, each processing element (PE) performs 1) a local computation (the blue part), 2) asynchronous messaging (the arrows), and 3) message handlers (the red part) in an interleaved fashion. We employ the actor model as a user-facing programming model to write a superstep as it inherently supports asynchronous messaging and message handling. </p> <p>One motivating example is the vertex-centric graph programming model. Specifically, in each superstep, each vertex asynchronously communicates to its neighbors over the edges via actor messaging:</p> vertex centric programming with the FA-BSP model (PSEUDO CODE!)<pre><code>// SPMD Execution (Each PE executes the same code)\n// superstep (User Code: local computation part)\n{\n    // for each vertex that belongs to the current PE\n    for (v: pe_local_vertices) {\n      // for each neighbor of the current vertex\n      for (neighbor: v.neighbors()) {\n        // asynchronously send a message to the neighbor\n        send(...); \n      }\n    }\n    // barrier and/or collective\n}\n...\n// User Code: Actor Definition\nclass MyActor {\n   ...\n   // The FA-BSP runtime calls this user-defined process function at a well-defined point\n   process(...) {\n     // process one message\n   }\n};\n</code></pre> <p>Here, in a superstep, each PE iterates over its local vertices' neighbors and sends a message to a specific neighbor with the <code>send()</code> API. The runtime switches back and forth between the superstep, message handler, and internal communication to overlap computation and communication.</p> <p>The FA-BSP model typically provides excellent scalability and performance and outperforms state-of-the-art BSP implementations in various large-scale graph applications.</p>"},{"location":"details/get/","title":"The remote get pattern","text":""},{"location":"details/get/#summary","title":"Summary","text":"<p>In the remote GET pattern, there are two communications: 1) a PE requests a remote PE to send back its local data (REQUEST), and 2) the receiver actually sends back data to the requester (REPLY). Since this GET communication is asynchronous, when the requester creates an asynchronous message, it also specifies where to store received data so the receiver can know the destination location in the requester's memory. </p> <p>The figure below illustrates the GET pattern. While the figure only shows one PE0 -&gt; PE1 -&gt; PE0 communication for presentation purposes, in reality, each PE does the sender's and receiver's role in an interleaved fashion.</p> <p> </p> The GET Pattern"},{"location":"details/get/#code-example","title":"Code Example","text":"<p>Actor class definition:</p> <pre><code>// packet\nstruct IgPkt {\n    int64_t dst;\n    int64_t src;\n};\n\nenum MailBoxType{REQUEST, REPLY};\n\nclass IgSelector: public hclib::Selector&lt;2, IgPkt&gt; {\n  //shared table array src, target\n  int64_t * ltable, *tgt;\n\n  void req_process(IgPkt pkt, int sender_rank) {\n      pkt.src = ltable[pkt.src];\n      // Reply back to the requester\n      send(REPLY, pkt, sender_rank);\n  }\n\n  void resp_process(IgPkt pkt, int sender_rank) {\n      tgt[pkt.dst] = pkt.src;\n  }\n\n  public:\n\n    IgSelector(int64_t *ltable, int64_t *tgt) : ltable(ltable), tgt(tgt){\n        mb[REQUEST].process = [this](IgPkt pkt, int sender_rank) { this-&gt;req_process(pkt, sender_rank); };\n        mb[REPLY].process = [this](IgPkt pkt, int sender_rank) { this-&gt;resp_process(pkt, sender_rank); };\n    }\n\n};\n</code></pre> <p>Main program:</p> <pre><code>IgSelector* actor_ptr = new IgSelector(larray1, larray2);\nhclib::finish([=]() {\n  actor_ptr-&gt;start();\n  for (int i = 0; i &lt; N; i++) {\n    IgPkt pkt;\n    pkt.dst = i;\n    pkt.src = i;\n    int pe = (shmem_my_pe() + 1) % shmem_n_pes();\n    printf(\"[PE%d MAIN] request PE%d to send ltable[%d]\\n\", shmem_my_pe(), pe, i);\n    actor_ptr-&gt;send(pkt, pe);\n  }\n  actor_ptr-&gt;done(REQUEST);\n});\n</code></pre>"},{"location":"details/put/","title":"The remote put pattern","text":""},{"location":"details/put/#summary","title":"Summary","text":"<p>In the remote PUT pattern, a PE requests a remote PE to update its local data. On the sender side, it keeps sending a massive number of requests to remote locations and invokes the <code>done()</code> API when it will not send any more messages. On the receiver side, the received messages are processed by a message handler one-by-one. </p> <p>One benefit of this communication model is that it does not require remote atomics (e.g., <code>shmem_atomic_add()</code>) due to the sequential processing of messages. </p> <p>The figure below illustrates the PUT pattern. While the figure only shows one PE0 to PE1 communication for presentation purposes, in reality, each PE does the sender's and receiver's role in an interleaved fashion.</p> <p> </p> The PUT Pattern"},{"location":"details/put/#code-example","title":"Code Example","text":"<p>Actor class definition:</p> <pre><code>// Actor Class\nclass MyActor: public hclib::Selector&lt;1, int&gt; {\n    int *larray;\n    // Message Handler\n    void process(int idx, int sender_rank) {\n        larray[idx] += 1;\n    }\npublic:\n    MyActor(int *larray) : larray(larray) {\n        mb[0].process = [this](int idx, int sender_rank) { this-&gt;process(idx, sender_rank);};\n    }\n};\n</code></pre> <p>Main program:</p> <pre><code>// Main Program\nMyActor* actor_ptr = new MyActor(larray);\nhclib::finish([=]() {\n    actor_ptr-&gt;start();\n    for (int i = 0; i &lt; N; i++) {\n        int pe = (shmem_my_pe() + 1) % shmem_n_pes();\n        // SEND\n        actor_ptr-&gt;send(i, pe);\n    }\n    actor_ptr-&gt;done(0);\n});\n</code></pre>"},{"location":"details/termination/","title":"Automatic Termination Detection","text":"<p>In general, the Actors/Selectors model provides an <code>exit()</code> operation to terminate actors/selectors. While it may seem somewhat natural to expose this operation to users, one problem with this termination semantics is that it requires users to ensure that all messages in the incoming mailbox are processed (or received in some cases) before invoking exit, which adds additional complexities even for the simplest kernel. </p> <p>To mitigate this burden, we added a relaxed version of <code>exit()</code>, which we call <code>done()</code>, to enable the runtime do more of the heavy lifting. The semantics of done is that users tell the runtime that the PE on which a specific actor/selector object resides will not send any more messages in the future to a particular mailbox, so the runtime can still keep the corresponding actor/selector alive so it can continue to receive messages and process them.</p>"},{"location":"details/yield/","title":"The yield() operation","text":"<p><code>hclib::yield()</code> defines a yield point for enabling context-switching between a main program and the selector runtime. While the <code>yield</code> operation is not always required, in some cases, the user has to explicitly call the operation to do the context-switching. Specifically, the <code>yield</code> operation may be required when a variable is shared by both the main program and the process method. </p> <p>First, consider the following selector class:</p> <pre><code>class MyActor: public hclib::Selector&lt;1, int&gt; {\n    int *larray;\n    int *lreceived;\n    void process(int idx, int sender_rank) {\n        larray[idx] += 1;\n        *lreceived = *lreceived + 1;\n    }\npublic:\n    MyActor(int *larray, int *lreceived) : larray(larray), lreceived(lreceived) {\n        mb[0].process = [this](int idx, int sender_rank) { this-&gt;process(idx, sender_rank);};\n    }\n};\n</code></pre> <p>Notice that <code>*lreceived</code> is incremented by one in the process method. Also, here is the main program:</p> <pre><code>int *received = (int*)calloc(1, sizeof(int));\nMyActor* actor_ptr = new MyActor(larray, received);\nhclib::finish([=]() {\n  actor_ptr-&gt;start();\n  for (int i = 0; i &lt; N; i++) {\n    int pe = (shmem_my_pe() + 1) % shmem_n_pes();\n    actor_ptr-&gt;send(i, pe);\n  }\n  // while (*received != N) { }\n  while (*received != N) { hclib::yield(); }\n  actor_ptr-&gt;done(0);\n});\nassert(*received == N);\n</code></pre> <p>On Line 9, there is an inactive while loop that loops until <code>*received == N</code>. Note that, if you comment in the line, the program will never be terminated due to the following reasons:</p> <p>First, recall that the main program and the process method are executed concurrently by the same PE in an interleaved fashion. Second, recall that the <code>send</code> API is non-blocking and it is not guaranteed that the operation is completed on the receiver side when <code>send</code> returns, which simultaneously means there is no guarantee that the process method is called <code>N</code> times when the execution reaches Line 9. </p> <p>Thus, on Line 9, if the while loop is commented in, the PE executing the while loop is blocked, which prevents the PE from exeucting the process method, which eventually prevents <code>lreceived</code> from getting updated. In most cases, we strongly recommend using <code>finish</code>, which ensures that all the send and process operations are completed. However, if there is a strong motivation for keeping track of <code>lreceived</code> in the finish scope, it is required to invoke <code>yield</code> in the while loop (Line 10) to let the runtime make progress on communications.</p>"},{"location":"getting_started/clusters/","title":"NERSC/ORNL/PACE","text":""},{"location":"getting_started/clusters/#prerequisites","title":"Prerequisites","text":"<ul> <li>a cluster/supercomputer with OpenSHMEM or UPC installed.</li> </ul> <p>As discussed in the background section, <code>hclib-actor</code> depends on Bale, which depends on either UPC or OpenSHMEM. Here we mainly explain steps to load OpenSHMEM, build bale, and build <code>hclib-actor</code> on three platforms: Perlmutter@NERSC, Cori@NERSC, Summit@ORNL, and PACE (Phoenix)@ GT.</p>"},{"location":"getting_started/clusters/#installationinitialization-scripts","title":"Installation/initialization Scripts","text":"Perlmutter@NERSCCori@NERSCSummit@ORNLPACE@GATech <p>perlmutter_setup.sh</p> <p>cori_setup.sh</p> <p>summit_setup.sh</p> <p>oshmem-slurm.sh</p> <p>Tip</p> <p>In order to run a job successfully every time after you login to a cluster/supercomputer, please make sure to</p> <ul> <li>Redirect to the directory where you initizally run the respective script for the platform using the above scripts.</li> <li><code>source</code> the script again to set all environment variables.</li> </ul>"},{"location":"getting_started/clusters/#run","title":"Run","text":"Perlmutter@NERSCCori@NERSCSummit@ORNLPACE@GATech <p>Example Slurm script (<code>example.slurm</code>)  example.slurm<pre><code>#!/bin/bash\n#SBATCH -q regular\n#SBATCH -N 2\n#SBATCH -C cpu\n#SBATCH -t 0:05:00\n\nsrun -n 256 ./histo_selector\n</code></pre> Submit a job <pre><code>sbatch example.slurm\n</code></pre></p> <p>Example Slurm script (<code>example.slurm</code>)  example.slurm<pre><code>#!/bin/bash\n#SBATCH -q regular               # job is submitted to regular queue\n#SBATCH -N 2                     # resources allocated, 2 nodes\n#SBATCH -C haswell               # use haswell nodes\n#SBATCH -t 00:30:00              # job will run at most 30min\n\nsrun -n 64 ./histo_selector\n</code></pre> Submit a job <pre><code>sbatch example.slurm\n</code></pre></p> <p>Example LSF script (<code>example.lsf</code>) example.lsf<pre><code>#!/bin/bash\n#BSUB -P XXXXX                     # project to which job is charged\n#BSUB -W 0:30                      # job will run at most 30 min\n#BSUB -nnodes 2                    # resources allocated, 2 nodes\n#BSUB -alloc_flags smt1            # one logical thread per physical core\n#BSUB -J histo                     # name of job\n#BSUB -o histo.%J                  # stdout file\n#BSUB -e histo.%J                  # stderror file\n\njsrun -n 84 ./histo_selector\n</code></pre> Submit a job <pre><code>bsub example.lsf\n</code></pre></p> <p>Example Slurm script(<code>example.sbatch</code>): example.sbatch<pre><code>#!/bin/bash\n#SBATCH -Joshmem                    # name of job\n#SBATCH --account=GT-XXXXXXX        # account to which job is charged\n#SBATCH -N 2                        # 2 nodes\n#SBATCH -n 48                       # resources allocated, 48 cores\n#SBATCH -t15                        # job will run at most 15mins\n#SBATCH -qinferno                   # job is submitted to inferno queue\n#SBATCH -ooshmem.out                # output file is named oshmem.out      \n\necho \"Started on `/bin/hostname`\"   # prints name of compute node job was started on\ncd $SLURM_SUBMIT_DIR                # changes into directory where script was submitted from\n\nsource ./oshmem-slurm.sh\n\ncd ./hclib/modules/bale_actor/test\nsrun -n 48 ./histo_selector\n</code></pre> Submit a job: <pre><code>sbatch example.sbatch\n</code></pre></p> <p>Example output:     <pre><code>Running histo on 48 threads\nbuf_cnt (number of buffer pkgs)      (-b)= 1024\nNumber updates / thread              (-n)= 1000000\nTable size / thread                  (-T)= 1000\nmodels_mask                          (-M)= 0\n   0.106 seconds\n</code></pre></p>"},{"location":"getting_started/clusters/#manual-installation-instructions","title":"Manual installation instructions","text":"<p>This part can be done with the Installation/initialization Scripts, but you can also manually install everything with the guide below.</p>"},{"location":"getting_started/clusters/#load-openshmem","title":"Load OpenSHMEM","text":"Perlmutter@NERSCCori@NERSCSummit@ORNLPACE@GATech <p>Use Cray OpenSHMEMX  <pre><code>module load cray-openshmemx\nmodule load cray-pmi\nexport PLATFORM=ex\nexport CC=cc\nexport CXX=CC\n</code></pre></p> <p>Use Cray SHMEM  <pre><code>module swap PrgEnv-intel PrgEnv-gnu\nmodule load cray-shmem \nmodule load python3\nexport PLATFORM=xc30\nexport CC=cc\nexport CXX=CC\n</code></pre></p> <p>Use OpenMPI's SHMEM (OSHMEM) <pre><code>module load python\nexport PLATFORM=oshmem\nexport CC=oshcc\nexport CXX=oshc++\n</code></pre></p> <p>Use OpenMPI's SHMEM (OSHMEM) <pre><code> module load gcc\n module load python\n module load openmpi/4.1.4\n export CC=oshcc\n export CXX=oshc++\n</code></pre></p> <p>Note</p> <p>You need to re-run the above commands every time you login to a cluster/supercomputer. You can use the respective script for the platform using the above pre-prepared scripts (<code>source ./{PLATFORM}_setup.sh</code> or <code>source ./oshmem-{PLATFORM}.sh</code>).</p>"},{"location":"getting_started/clusters/#build-bale-and-hclib","title":"Build Bale and HClib","text":""},{"location":"getting_started/clusters/#bale","title":"Bale","text":"<pre><code>git clone https://github.com/jdevinney/bale.git bale\ncd bale/src/bale_classic\nexport BALE_INSTALL=$PWD/build_${PLATFORM}\n./bootstrap.sh\npython3 ./make_bale -s\ncd ../../../\n</code></pre> <p>Note</p> <p>On Perlmutter, do <code>patch -p1 &lt; path/to/perlmutter.patch</code> in <code>bale</code> directory after <code>git clone</code>. You can find <code>perlmutter.patch</code> here.</p> <p>Note</p> <p>Bale will be installed in <code>bale/src/bale_classic/build_${PLATFORM}</code></p>"},{"location":"getting_started/clusters/#hclib","title":"HClib","text":"<pre><code>git clone https://github.com/srirajpaul/hclib\ncd hclib\ngit fetch &amp;&amp; git checkout bale3_actor\n./install.sh\nsource hclib-install/bin/hclib_setup_env.sh\ncd modules/bale_actor &amp;&amp; make\ncd test\nunzip ../inc/boost.zip -d ../inc/\nmake\ncd ../../../../\n</code></pre>"},{"location":"getting_started/clusters/#setting-environment-variables","title":"Setting environment variables","text":"<pre><code>export BALE_INSTALL=$PWD/bale/src/bale_classic/build_${PLATFORM}\nexport HCLIB_ROOT=$PWD/hclib/hclib-install\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$BALE_INSTALL/lib:$HCLIB_ROOT/lib:$HCLIB_ROOT/../modules/bale_actor/lib\nexport HCLIB_WORKERS=1\n</code></pre>"},{"location":"getting_started/docker/","title":"Docker","text":""},{"location":"getting_started/docker/#prerequisites","title":"Prerequisites","text":"<ul> <li>a windows/linux/mac machine with <code>docker</code> installed.</li> </ul> <p>Info</p> <p>Apple's M1/M2/M3-based machines are now supported.</p> <p>Warning</p> <p>This docker environment is primarily for testing and is not supposed to be used for performance runs. For performance runs, see the clusters/supercomputer section.</p>"},{"location":"getting_started/docker/#download-the-dockerfile","title":"Download the <code>Dockerfile</code>","text":"<p>Currently, there are two versions of <code>Dockerfile</code>: one is based on the latest version of the bale library (3.x) and another is based on bale 2.x. We would recommend using the <code>bale3</code> version unless there is a specific reason to use <code>bale_old</code> version.</p> For bale3For bale_old <ul> <li>Dockerfile for bale3</li> </ul> <ul> <li>Dockerfile for bale_old</li> </ul>"},{"location":"getting_started/docker/#build-the-dockerfile","title":"Build the <code>Dockerfile</code>","text":"<pre><code>docker build -t actor -f Dockerfile .\n</code></pre> <p>Note</p> <p>For more details on <code>docker build</code>, please see the official document</p>"},{"location":"getting_started/docker/#run-the-container","title":"Run the container","text":"<pre><code>docker run -it actor /bin/bash\n</code></pre> <p>Note</p> <p>For more details on <code>docker run</code>, please see the official document</p> <p>Tip</p> <p>Once you run the container, you can safely <code>exit</code> from it and may want to <code>stop</code> it by doing <code>docker stop</code> unless you gave the <code>--rm</code> option to the <code>docker run</code> command, which destroy the container when you exit. When you resume, you first need to make sure the status of the container by doing <code>docker ps -a</code>: </p> <pre><code>// On the host\n$ docker ps -a\nCONTAINER ID   IMAGE           COMMAND       CREATED         STATUS                       PORTS                    NAMES\n145199093e17   actor           \"/bin/bash\"   3 weeks ago     Exited (0) 2 weeks ago                                ecstatic_snyder\na831e3f73cec   actor           \"/bin/bash\"   15 months ago   Up 27 hours                                           priceless_franklin\n</code></pre> <p>If the container is \"Up\", you can attach to it by doing <code>docker exec -it [CONTAINER NAME] /bin/bash</code>. In this example, <code>priceless_franklin</code> is up and you can attach to it by doing <code>docker exec -it priceless_franlkin /bin/bash</code>. Otherwise, you first need to start the container by doing <code>docker start [CONTAINER NAME]</code>. </p> <p>Within the container, the following environment variables are defined </p> Var Value Description LOCAL /root/local The location of the OpenSHMEM toolchain is installed CC /root/local/bin/oshcc The OpenSHMEM C compiler CXX /root/local/bin/oshc++ The OpenSHMEM C++ compiler OSHRUN /root/local/bin/oshrun The OpenSHMEM launcher BALE_INSTALL /root/bale/build_unknown The location of the Bale library HCLIB_ROOT /root/hclib/hclib-install The location of the HClib LD_LIBRARY_PATH LD_LIBRARY_PATH=$LOCAL/lib:$BALE_INSTALL/lib:$HCLIB_ROOT/lib:$HCLIB_ROOT/../modules/bale_actor/lib The locations of static/dynamic libraries HCLIB_WORKERS 1 The number of HClib workers per each PE <p>Warning</p> <p>Do NOT change the value of <code>HCLIB_WORKER</code>. In the current implementation, we exploit the OpenSHMEM PE-level parallelism, where each PE is associated with a physical/virtual CPU core, and creating multiple workers per PE can degrade the performance and cause an error.</p>"},{"location":"getting_started/docker/#run-the-histogram-example","title":"Run the histogram example","text":"<p>Now that the container is running, let's build and run the selector version of the histogram benchmark. You can make it by doing <code>make histo_selector</code> and  launch it with 2 PEs using <code>$OSHRUN</code>. </p> <pre><code>// move to \"test\" directory\ncd ../test\n// build a selector version of histogram\nmake histo_selector\n// run it on 2PE using $OSHRUN\n$OSHRUN -n 2 ./histo_selector -n 100\n</code></pre> <p>Example Output: <pre><code>WARNING: Failed dynamically loading /root/hclib/hclib-install/lib/libhclib_bale_actor.so for \"bale_actor\" dependency\nWARNING: HCLIB_LOCALITY_FILE not provided, generating sane default locality information\nWARNING: HCLIB_WORKERS provided, creating locale graph based on 1 workers\nWARNING: Failed dynamically loading /root/hclib/hclib-install/lib/libhclib_bale_actor.so for \"bale_actor\" dependency\nWARNING: HCLIB_LOCALITY_FILE not provided, generating sane default locality information\nWARNING: HCLIB_WORKERS provided, creating locale graph based on 1 workers\nRunning histo on 2 threads\nbuf_cnt (number of buffer pkgs)      (-b)= 1024\nNumber updates / thread              (-n)= 100\nTable size / thread                  (-T)= 1000\nmodels_mask                          (-M)= 0\n     0.019 seconds\n</code></pre></p> <p>Note</p> <p>Recall that OSHRUN is an environment variable and do not forget to add the dollar sign ($) before OSHRUN.</p> <p>Note</p> <p>Lambda versions may not work on the docker environment due to the address space layout randomization.</p>"},{"location":"getting_started/singularity/","title":"Singularity","text":"<p>This document discusses setting up a singularity container both locally and on PACE (Georgia Tech cluster resources).</p>"},{"location":"getting_started/singularity/#setting-up-singularity-locally","title":"Setting up Singularity Locally","text":""},{"location":"getting_started/singularity/#prerequisites","title":"Prerequisites","text":"<ul> <li>a windows/linux/mac machine with <code>singularity</code> installed.</li> </ul> <p>Warning</p> <p>This singularity environment is primarily for testing and is not supposed to be used for performance runs. For performance runs, see the clusters/supercomputer section.</p>"},{"location":"getting_started/singularity/#download-the-singularity-definition-file","title":"Download the <code>Singularity Definition File</code>","text":"<p>Currently, there are two versions of <code>Definition File</code>: one is based on the latest version of the bale library (3.x) and another is based on bale 2.x. We would recommend using the <code>bale3</code> version unless there is a specific reason to use <code>bale_old</code> version.</p> For bale3For bale_old <ul> <li>Def File for bale3</li> </ul> <ul> <li>Def File for bale_old</li> </ul>"},{"location":"getting_started/singularity/#build-the-singularity-def-file","title":"Build the <code>Singularity Def File</code>","text":"<pre><code>sudo singularity build --sandbox actor Singularity.def\n</code></pre> <p>Note</p> <p>This creates a sandbox container to allow for read-write operations. For more details on <code>singularity build</code>, please see the official document</p>"},{"location":"getting_started/singularity/#use-the-container","title":"Use the container","text":"<pre><code>sudo singularity shell actor\n</code></pre> <p>Note</p> <p>For more details on <code>singularity shell</code>, please see the official document</p> <p>Tip</p> <p>Once you run the container, you can safely <code>exit</code> from it and automatically keep any edits made. You can resume the session by using the same above command.</p> <p>Within the container, the following environment variables are defined </p> Var Value Description LOCAL /usr/local The location of the OpenSHMEM toolchain is installed CC /usr/local/bin/oshcc The OpenSHMEM C compiler CXX /usr/local/bin/oshc++ The OpenSHMEM C++ compiler OSHRUN /usr/local/bin/oshrun The OpenSHMEM launcher BALE_INSTALL /usr/local/bale/build_unknown The location of the Bale library HCLIB_ROOT /usr/hclib/hclib-install The location of the HClib LD_LIBRARY_PATH LD_LIBRARY_PATH=$LOCAL/lib:$BALE_INSTALL/lib:$HCLIB_ROOT/lib:$HCLIB_ROOT/../modules/bale_actor/lib The locations of static/dynamic libraries HCLIB_WORKERS 1 The number of HClib workers per each PE <p>Warning</p> <p>Do NOT change the value of <code>HCLIB_WORKER</code>. In the current implementation, we exploit the OpenSHMEM PE-level parallelism, where each PE is associated with a physical/virtual CPU core, and creating multiple workers per PE can degrade the performance and cause an error.</p>"},{"location":"getting_started/singularity/#run-the-histogram-example","title":"Run the histogram example","text":"<p>Now that the container is running, let's build and run the selector version of the histogram benchmark. You can make it by doing <code>make histo_selector</code> and  launch it with 2 PEs using <code>$OSHRUN</code>. </p> <pre><code>cd actor/usr/local/hclib/modules/bale_actor/test\nmake histo_selector\n$OSHRUN -n 2 ./histo_selector -n 100                                                                                                                             WARNING: Failed dynamically loading /usr/local/hclib/hclib-install/lib/libhclib_bale_actor.so for \"bale_actor\" dependency\nWARNING: HCLIB_LOCALITY_FILE not provided, generating sane default locality information\nWARNING: HCLIB_WORKERS provided, creating locale graph based on 1 workers\nWARNING: Failed dynamically loading /usr/local/hclib/hclib-install/lib/libhclib_bale_actor.so for \"bale_actor\" dependency\nWARNING: HCLIB_LOCALITY_FILE not provided, generating sane default locality information\nWARNING: HCLIB_WORKERS provided, creating locale graph based on 1 workers\nRunning histo on 2 threads\nbuf_cnt (number of buffer pkgs)      (-b)= 1024\nNumber updates / thread              (-n)= 100\nTable size / thread                  (-T)= 1000\nmodels_mask                          (-M)= 0\n     0.719 seconds\n</code></pre> <p>Note</p> <p>Recall that OSHRUN is an environment variable and do not forget to add the dollar sign ($) before OSHRUN.</p>"},{"location":"getting_started/singularity/#setting-up-singularity-on-pacegatech","title":"Setting up Singularity on PACE@GATech","text":""},{"location":"getting_started/singularity/#setting-up-the-environment","title":"Setting up the environment","text":"<p>Load the following modules into the PACE environment:</p> <pre><code>module load pace-community\nmodule load hclib-pace\n</code></pre>"},{"location":"getting_started/singularity/#use-the-container_1","title":"Use the container","text":"<pre><code>singularity shell ${HCLIB_PACE_SIF}\n</code></pre> <p>Note</p> <p>For more details on <code>singularity shell</code>, please see the official document</p> <p>Note</p> <p>This is a read-only container, meaning all the libraries and benchmarks have already been precompiled and are ready for use (see section regarding running benchmarks).</p> <p>Tip</p> <p>Once you run the container, you can safely <code>exit</code> from it.</p> <p>Within the container, the following environment variables are defined </p> Var Value Description LOCAL /usr/local The location of the OpenSHMEM toolchain is installed CC /usr/local/bin/oshcc The OpenSHMEM C compiler CXX /usr/local/bin/oshc++ The OpenSHMEM C++ compiler OSHRUN /usr/local/bin/oshrun The OpenSHMEM launcher BALE_INSTALL /usr/local/bale/build_unknown The location of the Bale library HCLIB_ROOT /usr/hclib/hclib-install The location of the HClib LD_LIBRARY_PATH LD_LIBRARY_PATH=$LOCAL/lib:$BALE_INSTALL/lib:$HCLIB_ROOT/lib:$HCLIB_ROOT/../modules/bale_actor/lib The locations of static/dynamic libraries HCLIB_WORKERS 1 The number of HClib workers per each PE <p>Warning</p> <p>Do NOT change the value of <code>HCLIB_WORKER</code>. In the current implementation, we exploit the OpenSHMEM PE-level parallelism, where each PE is associated with a physical/virtual CPU core, and creating multiple workers per PE can degrade the performance and cause an error.</p>"},{"location":"getting_started/singularity/#run-the-histogram-example_1","title":"Run the histogram example","text":"<p>Now that the container is running, let's build and run the selector version of the histogram benchmark. It has been precompiled so you can launch it with 2 PEs using <code>$OSHRUN</code>. </p> <pre><code>cd actor/usr/local/hclib/modules/bale_actor/test\n$OSHRUN -n 2 ./histo_selector -n 100                                                                                                                             WARNING: Failed dynamically loading /usr/local/hclib/hclib-install/lib/libhclib_bale_actor.so for \"bale_actor\" dependency\nWARNING: HCLIB_LOCALITY_FILE not provided, generating sane default locality information\nWARNING: HCLIB_WORKERS provided, creating locale graph based on 1 workers\nWARNING: Failed dynamically loading /usr/local/hclib/hclib-install/lib/libhclib_bale_actor.so for \"bale_actor\" dependency\nWARNING: HCLIB_LOCALITY_FILE not provided, generating sane default locality information\nWARNING: HCLIB_WORKERS provided, creating locale graph based on 1 workers\nRunning histo on 2 threads\nbuf_cnt (number of buffer pkgs)      (-b)= 1024\nNumber updates / thread              (-n)= 100\nTable size / thread                  (-T)= 1000\nmodels_mask                          (-M)= 0\n     0.719 seconds\n</code></pre> <p>Note</p> <p>Recall that OSHRUN is an environment variable and do not forget to add the dollar sign ($) before OSHRUN.</p>"},{"location":"history/history/","title":"History","text":"<ul> <li>Preview version is released (Dec 2022)</li> </ul>"},{"location":"tools/actorprof/","title":"ActorProf: A Framework for Profiling and Visualizing Fine-grained Asynchronous Bulk Synchronous Parallel Execution","text":"<p>This document gives a brief guidance on how to generate trace for actor applications on COTS (typically x86) systems using ActorProf. For more details about ActorProf, please refer to our paper.</p>"},{"location":"tools/actorprof/#step-by-step-guide","title":"Step-by-Step Guide","text":"<p>Here we will take Triangle Counting selector as an example on <code>Perlmutter</code>.For other machines, please follow the Manual installation and run instructions</p>"},{"location":"tools/actorprof/#step-1-build-hclib-and-bale","title":"Step 1: Build <code>HClib</code> and <code>Bale</code>","text":"<p>Build the HClib and bale libraries to setup the environment. For setting up <code>HClib</code> and <code>Bale</code> libraries on <code>Perlmutter</code>, please source the <code>perlumtter_setup.sh</code> script provided. <pre><code>source perlumtter_setup.sh\n</code></pre> Please refer to HClib-Actor setup page for more details on how to setup on other machines</p> <p>Note: please re-direct to current directory and source the setup script again to set all environment variables every time after you login to a cluster/supercomputer</p>"},{"location":"tools/actorprof/#step-2-allocate-interactive-compute-node","title":"step 2: Allocate interactive compute node","text":"<p>Allocate to run the Actorprof scripts</p> <pre><code>salloc --nodes 1 --qos interactive --time 00:10:00 --constraint cpu --account=mxxx\n</code></pre>"},{"location":"tools/actorprof/#step-3-run-actorprof-script","title":"Step 3: Run Actorprof Script","text":"<p>Run the ActorProf bash script (<code>run_actorprof.sh</code>) which has 4 options:</p> <pre><code>   source ./run_actorprof.sh [logical | papi | physical| overall | all] [triangle_selector | triangle_selector_interval] [1...N] [1...N]\n\n   [logical | papi | physical | overall | all]                       Selects which type of trace (or all) to generate\n   [triangle_selector | triangle_selector_interval] Selects which application to generate the trace (triangle_selector - cyclic distribution or triangle_selector_interval - range distribution)\n   [1...N]                                        Selects Scale of the RMATE graph\n   [1...N]                                        Selects the number of cores for the run\n</code></pre>"},{"location":"tools/actorprof/#logical-trace","title":"Logical Trace","text":"<p>E.g. Generate logical trace of triangle selector with 1D Cyclic distribution on scale of 10 using 2 cores <pre><code>source run_actorprof.sh logical triangle_selector 10 2\n</code></pre></p> <p>It will generate one trace file <code>*send.csv</code> for each PE and a Heatmap <code>application_logical.png</code>. In this example, three files <code>PE0_send.csv</code>, <code>PE1_send.csv</code> and <code>triangle_selector_logical.png</code> were generated since we ran this application on two threads.</p>"},{"location":"tools/actorprof/#hwpc-trace","title":"HWPC Trace","text":"<p>E.g. Generate HWPC trace of triangle selector with 1D Cyclic distribution on scale of 10 using 2 cores <pre><code>source run_actorprof.sh papi triangle_selector 10 2\n</code></pre></p> <p>It will generate two trace file <code>PE*_send.csv</code> and <code>PE*_papi.csv</code> for each PE, one Heatmap <code>logical.png</code>, and a bar graph <code>papi.png</code>. In this example, three files <code>PE0_send.csv</code>, <code>PE1_send.csv</code>, <code>logical.png</code>, and <code>papi.png</code> were generated since we ran this application on two threads.</p>"},{"location":"tools/actorprof/#physical-trace","title":"Physical trace","text":"<p>E.g. Generate physical trace of triangle selector with 1D Cyclic distribution on scale of 10 using 2 cores <pre><code>source run_actorprof.sh physical triangle_selector 10 2\n</code></pre></p> <p>It will generate one trace file <code>physical.txt</code>  and a stacked bar graph <code>physical.png</code>.</p>"},{"location":"tools/actorprof/#overall-trace","title":"Overall Trace","text":"<p>E.g. Generate overall trace of triangle selector with 1D Cyclic distribution on scale of 10 using 2 cores <pre><code>source run_actorprof.sh overall triangle_selector 10 2\n</code></pre></p> <p>It will generate the <code>overall.txt</code> trace file and a stacked bar graph <code>overall.png</code>.</p> <p>Note: user can use <code>all</code> to generat all four trace mentioned above at once</p>"},{"location":"tools/actorprof/#manual-installation-and-run-instructions","title":"Manual installation and run instructions\u00b6","text":"<p>If user decide to build and run Actorprof manually without using the ActorProf bash script (<code>run_actorprof.sh</code>), you can use the guide below. </p>"},{"location":"tools/actorprof/#step-1-environment-setup","title":"Step 1: Environment Setup","text":"<p>Please refer to HClib-Actor setup page for more details on how to build the HClib and bale libraries to setup the environment.</p>"},{"location":"tools/actorprof/#step-2-build-application-with-trace-flag-enabled","title":"Step 2: Build Application with trace flag enabled","text":"<ul> <li><code>-DENABLE_TRACE</code> flag for enabling logical message generation macro.</li> <li><code>-DENABLE_TRACE_PAPI</code> flag for enabling logical message and HWPC trace generation macro.</li> <li><code>-DENABLE_TCOMM_PROFILING</code> flag for enabling overall trace generation macros.</li> <li><code>-DENABLE_TRACE_PHYSICAL</code> flag for enabling physical message trace generation macro.</li> </ul> <p>Below is an example of building the 1D-Cyclic Triangle Counting application on Perlmutter with logical, overall, and physical trace macros respectively using <code>Makefile</code>. <pre><code>cd $PWD/hclib/modules/bale_actor/test\nmake triangle_selector_logical\nmake triangle_selector_papi\nmake triangle_selector_overall\nmake triangle_selector_physical\n</code></pre></p>"},{"location":"tools/actorprof/#step-3-trace-generation","title":"Step 3: Trace Generation","text":"<p>Here we will take 1D-Cyclic Triangle Counting  to run as an example on <code>Perlmutter</code> interactive node.</p> <p>1) To generate Logical Message Trace and HWPC Trace <pre><code>srun  -N 2 -n 32 --cpu-bind=cores ./triangle_selector_papi -f small.mtx\n</code></pre></p> <p>It will generate two trace files (<code>*send.csv</code> and <code>*PAPI.csv</code>) for each PE.  In this example, 64 data files, i.e., <code>PE0_send.csv</code>, <code>PE0_PAPI.csv</code>, <code>PE1_send.csv</code>, <code>PE1_PAPI.csv</code>,..., will be generated since we ran this application on 32 threads.</p> <p>Note: To generate logical trace only, please use triangle_selector_logical executable.</p> <p>2) To generate Overall Trace <pre><code>srun  -N 2 -n 32 --cpu-bind=cores ./triangle_selector_overall -f small.mtx &amp;&gt;overall.txt\n</code></pre></p> <p><code>overall.txt</code> contains overall trace for every PE in one <code>.txt</code> file.</p> <p>3) To generate Physical Message Trace <pre><code>srun  -N 2 -n 32 --cpu-bind=cores ./triangle_selector_physical -f small.mtx &amp;&gt; physical.txt\n</code></pre></p> <p><code>physical.txt</code> contains Physical message trace for every PE in one <code>.txt</code> file.</p>"},{"location":"tools/actorprof/#step-4-actorprof-visualization","title":"Step 4: ActorProf Visualization","text":"<p>Four type of graphs can be generated with ActorProf with different flags using <code>actorprof.py</code>, please put all generated trace into the data directory before running ActorProf.</p> <p><code>transfer.sh</code> can be used to create data dir in correct format and move all generated trace into the data directory.  <pre><code>source transfer.sh\n</code></pre></p> <p>Cautious: Please use the script or manually empty/remove the data directory every time generating a new trace file to aviod trace data overlap, which may lead to incorrect visualizing result.</p> <p>Path to the data directory (<code>path</code>) and total number of PEs( <code>-n</code> or <code>--num_PEs</code> ) used to generate the trace files are required for running the ActorProf. 1) Logical Message trace Heatmap <code>-l</code> flag is needed to generate Logical Message Trace Heatmap 2) Physical Message trace Heatmap <code>-p</code> option is needed to generate Physical Message Trace Heatmap \u2003 <code>0</code>for Local Send Message trace Heatmap (Default) \u2003 <code>1</code> for Non-blocking Message Trace Heatmap 3) HWPC trace Heatmap <code>-lp</code> flag is needed to generate HWPC Trace bar-graph 4) Overall trace Heatmap <code>-s</code> flag is needed to generate stacked bar-graph for overall absolute and relative execution time</p> <p>Note: please specify all flags when trying to profile all result.</p> <p>Example to run ActorProf visualizer using <code>actorprof.py</code> to generate physical trace Heatmap. <pre><code>python actorprof.py ./data -n 32 -p\n</code></pre></p> <p>All result will be saved as an  <code>.png</code> figure.</p>"},{"location":"tools/actorprof/#top-level-directory-organization","title":"Top-Level Directory Organization","text":"<p>The folder structure of this repository is as follows:</p> <pre><code>.\n\u251c\u2500\u2500 ActorProf           # Contains files for the ActorProf Tool\n\u2502   \u251c\u2500\u2500 hclib           # Contains the HClib library and the  Actor-based runtime\n\u2502   \u2502   \u251c\u2500\u2500 ...                                         \n\u2502   \u2514\u2500\u2500 \u2500\u2500\u2500 modules                         \n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ...                             \n\u2502   \u2514\u2500\u2500 \u2500\u2500\u2500 \u2500\u2500\u2500 bale_actor                       \n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 ...                                \n\u2502   \u2514\u2500\u2500 \u2500\u2500\u2500 \u2500\u2500\u2500 \u2500\u2500\u2500 test    # Contains the Triangle Counting Selector application files\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 triangle_selector.cpp   # Triangle Counting code for 1D-Cyclic version\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 triangle_selector_interval.cpp  # Triangle Counting code for 1D-Range version\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 small.mtx                       # Scale of 16 Triangle Counting graph\n\u2502   \u2514\u2500\u2500 \u2500\u2500\u2500 \u2500\u2500\u2500 \u2500\u2500\u2500 ...                             \n\u251c\u2500\u2500 logical.py          # Visualization for Logical Message Trace\n\u251c\u2500\u2500 papi.py             # Visualization for HWPC Trace\n\u251c\u2500\u2500 physical.py         # Visualization for Physical Message Trace\n\u251c\u2500\u2500 overall.py          # Visualization for Overall Trace    \n\u251c\u2500\u2500 generate_rmate.py   # RMAT Graph generation for applications\n\u251c\u2500\u2500 run_actorprof.sh    # Top to down complete run script for using ActorProf \n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"tools/actorprof/#citation","title":"Citation","text":"<p>If you use our application in your work, please cite our paper.</p> <p>ActorProf: A Framework for Profiling and Visualizing Fine-grained Asynchronous Bulk Synchronous Parallel Execution. Jiawei Yang, Shubhendra Pal Singhal, Jun Shirako, Akihiro Hayashi, Vivek Sarkar. Workshop on Programming and Performance Visualization Tools (ProTools2024, co-located with SC24)</p> <p>Corresponding author: Jiawei Yang (jyang810@gatech.edu), Shubhendra Pal Singhal(ssinghal74@gatech.edu)</p>"},{"location":"tools/craypat/","title":"CrayPat","text":""},{"location":"tools/craypat/#introduction","title":"Introduction","text":"<p>CrayPat (Cray Performance Measurement and Analysis toolset) is Cray\u2019s performance analysis tool offered by Cray. Since CrayPat is only available on Cray systems, let us give a brief step-by-step guidance on how to use it on Perlmutter.</p>"},{"location":"tools/craypat/#step-by-step-guide","title":"Step-by-Step Guide","text":"<p>Here we will take Triangle Counting selector as an example:</p>"},{"location":"tools/craypat/#step-0-load-compilers","title":"Step 0: Load compilers","text":"<p>It is important to load compiler modules before Step 1. In our case, <code>source</code> perlmutter_setup.sh: <pre><code>source ./perlmutter_setup.sh\n</code></pre></p>"},{"location":"tools/craypat/#step-1-unloadload-required-modules","title":"Step 1: Unload/Load required modules","text":"<pre><code>module unload darshan\nmodule load perftools-base perftools\n</code></pre>"},{"location":"tools/craypat/#step-2-build-the-application-as-normal-but-keep-o-files","title":"Step 2: Build the application as normal but keep <code>.o</code> files","text":"<p>Go to the <code>test</code> directory to separately create an object file and executable for the triangle counting code: <pre><code>cd $HCLIB_ROOT/../modules/bale_actor/test/\n</code></pre> Create a tringle counting object file (.o file): <pre><code>CC -g -O3 -std=c++11 -DUSE_SHMEM=1 -I$HCLIB_ROOT/include -I$BALE_INSTALL/include -I$HCLIB_ROOT/../modules/bale_actor/inc -L$HCLIB_ROOT/lib -L$BALE_INSTALL/lib -L$HCLIB_ROOT/../modules/bale_actor/lib -c -o triangle_selector.o triangle_selector.cpp -lhclib -lrt -ldl -lspmat -lconvey -lexstack -llibgetput -lhclib_bale_actor -lm\n</code></pre> Build a triangle counting executable file: <pre><code>CC -g -O3 -std=c++11 -DUSE_SHMEM=1 -I$HCLIB_ROOT/include -I$BALE_INSTALL/include -I$HCLIB_ROOT/../modules/bale_actor/inc -L$HCLIB_ROOT/lib -L$BALE_INSTALL/lib -L$HCLIB_ROOT/../modules/bale_actor/lib -o triangle_selector  triangle_selector.o -lhclib -lrt -ldl -lspmat -lconvey -lexstack -llibgetput -lhclib_bale_actor -lm\n</code></pre></p> <p>Tip</p> <p>It is recommended to do <code>make triangle_selector</code> first to see the full compilation command.</p> <ul> <li>First, copy and paste the full command and change <code>-o triangle_selector triangle_selector.cpp</code> to <code>-c triangle_selector triangle_selector.cpp</code> to create an object file (.o file). </li> <li>Second, copy and paste the command again and change <code>-o triangle_selector triangle_selector.cpp</code> to <code>-o triangle_selector triangle_selector.o</code> to create an executable.</li> </ul>"},{"location":"tools/craypat/#step-3-instrument-the-application-using-pat_build","title":"Step 3: Instrument the application using <code>pat_build</code>","text":"<p>Generate a CrayPat instrumented executable using <code>pat_build</code>: <pre><code>pat_build -w -g shmem triangle_selector\n</code></pre></p> <p>Note</p> <p>By default, <code>triangle_selector+pat</code> is generated.</p> <p>Note</p> <p><code>pat_build</code> has different option that can trace a specific function(s)</p> <ul> <li><code>-g</code>: trace Cray-provided library function group such as MPI and OpenSHMEM</li> <li><code>-u</code>: trace all user functions routine by routine</li> <li><code>-w</code>: flag that enables tracing</li> <li><code>-T -w</code>: trace user-defined functions<ul> <li>e.g, <code>pat_build -w -T selector_function</code></li> </ul> </li> </ul>"},{"location":"tools/craypat/#step-4-run-the-instrumented-executable-to-get-performance-data","title":"Step 4: Run the instrumented executable to get performance data","text":""},{"location":"tools/craypat/#option-1-run-the-executable-with-an-interactive-batch-job","title":"Option 1: Run the executable with an interactive batch job","text":"<p>Allocating resources: <pre><code>salloc --nodes 1 --qos interactive --time 00:05:00 --constraint cpu --account=mxxx\n</code></pre> Run the excutable to generate a directory (e.g., <code>triangle_selector+pat+174621-8716327t</code>) containing performance data files with the<code>.xf</code> suffix: <pre><code>srun  -n 128 --cpu-bind=cores ./triangle_selector+pat\n</code></pre></p>"},{"location":"tools/craypat/#option-2-run-the-executable-with-sbatch-script","title":"Option 2: Run the executable with sbatch script","text":"<pre><code>#!/bin/bash\n#SBATCH -q regular\n#SBATCH -N 1\n#SBATCH -C cpu\n#SBATCH -t 0:05:00\n#SBATCH -ooshmem_%j_tri_cout.out\n\nsource ./oshmem-perlmutter.sh\n\nmodule unload darshan\nmodule load perftools-base perftools\nexport LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH\n\ncd $HOME/hclib/modules/bale_actor/test\necho \"--------------------------------------------\"\nsrun  -n 128 --cpu-bind=cores ./triangle_selector+pat\necho \"--------------------------------------------\"\n</code></pre>"},{"location":"tools/craypat/#step-5-generate-human-readable-content-with-pat_report","title":"Step 5: Generate human-readable content with <code>pat_report</code>","text":"<p>Run <code>pat_report</code> with the generated directory name, which will output a text report on the terminal and creates files with different suffices, <code>.ap2</code> and <code>.apa</code> inside the directory: <pre><code>pat_report ./triangle_selector+pat+174621-8716327t\n</code></pre></p>"},{"location":"tools/craypat/#exemplar-text-report","title":"Exemplar text report","text":"<pre><code>CrayPat/X:  Version 23.03.0 Revision 46f710008  02/13/23 20:24:04\nNumber of PEs (MPI ranks):   128\nNumbers of PEs per Node:     128\nNumbers of Threads per PE:     1\nNumber of Cores per Socket:   64\nExecution start time:  Fri Apr 14 13:44:36 2023\nSystem name and speed:  nid004675  2.671 GHz (nominal)\nAMD   Milan                CPU  Family: 25  Model:  1  Stepping:  1\nCore Performance Boost:  All 128 PEs have CPB capability\nCurrent path to data file:\n  /Users/joseph/triangle_selector+pat+197953-8716330t   (RTS, 128 data files)\nNotes for table 1:\n  This table shows functions that have significant exclusive time,\n    averaged across ranks.\n  For further explanation, see the \"General table notes\" below, or \n    use:  pat_report -v -O profile ...\nTable 1:  Profile by Function Group and Function\n  Time% |     Time |     Imb. |  Imb. | Calls | Group\n        |          |     Time | Time% |       |  Function\n        |          |          |       |       |   PE=HIDE\n 100.0% | 6.079316 |       -- |    -- | 404.0 | Total\n|-------------------------------------------------------------------\n|  95.2% | 5.789928 |       -- |    -- |   2.0 | USER\n||------------------------------------------------------------------\n||  47.7% | 2.901679 | 0.292158 |  9.2% |   1.0 | main\n||  47.5% | 2.888249 | 0.000057 |  0.0% |   1.0 | #1.selector_function\n||==================================================================\n|   4.8% | 0.289263 | 0.100863 | 26.1% |   2.0 | DL\n||------------------------------------------------------------------\n||   4.8% | 0.289263 | 0.100863 | 26.1% |   2.0 | dlopen\n|===================================================================\nNotes for table 2:\n  This table shows functions that have the most significant exclusive\n    time, taking the maximum time across ranks and threads.\n  For further explanation, see the \"General table notes\" below, or \n    use:  pat_report -v -O profile_max ...\nTable 2:  Profile of maximum function times\n  Time% |     Time |     Imb. |  Imb. | Function\n        |          |     Time | Time% |  PE=[max,min]\n|-----------------------------------------------------------\n| 100.0% | 3.193838 | 0.292158 |  9.2% | main\n...\n</code></pre> <p>Note</p> <p><code>.ap2</code> is used to view performance data graphically with the Cray Apprentice2 tool. <code>.apa</code> is for suggested <code>pat_build</code> options for more detailed tracing experiments.</p>"},{"location":"tools/craypat/#using-apprentice2-for-analyzing-results","title":"Using Apprentice2 for analyzing results","text":"<p>Cray Apprentice2 is a GUI-based analysis tool that can be used to visualize performance data instrumented with the CrayPat tool. Cray offers a desktop version of the Cray Apprentice2 visualizer so you can do your analysis locally.</p> <p>To install a desktop version, you can find the installer on Perlmutter as below: <code>$CRAYPAT_ROOT/share/desktop_installers</code></p> <p><code>scp</code> an appropriate installer to your local machine and install it. After that, you will be able to open <code>.ap2</code> file with Apprentice2.</p> <p>Tips</p> <p>If you encounter this error: <code>/some/path/./a.out: error while loading shared libraries: pat.so: cannot open shared object file: No such file or directory</code> </p> <p>Try this: <code>export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH</code></p>"},{"location":"tools/craypat/#specifying-profiling-region","title":"Specifying profiling region","text":"<p>CrayPat performance API can be use to identify the region of interest (ROG) for analysis.</p> <ul> <li><code>PAT_record(int state)</code></li> <li>Setting the recording state to PAT_STATE_ON or PAT_STATE_OFF</li> <li>Needs to be inserted before the ROG</li> <li><code>PAT_region_begin(int id, char *label)</code></li> <li>Defines the boundaries of a region</li> <li>Needs to be inserted at the start the ROG</li> <li><code>PAT_region_end(int id);</code></li> <li>regions must be either separate or nested</li> <li>Needs to be inserted at the end of ROG</li> </ul>"},{"location":"tools/craypat/#example","title":"Example","text":"<p><pre><code>#include &lt;pat_api.h&gt;\n...\nPAT_record(PAT_STATE_ON);\nPAT_region_begin(1,\"selector_function\");\ntriangle_selector();\nPAT_region_end(1);\n...\n</code></pre> After generating the instrumented excutable with <code>pat_build</code> and run it, CrayPat will generate a trace of ROG in the performance data which can be found in the <code>pat_report</code></p> <p>Exemplar output <pre><code>Table 1:  Profile by Function Group and Function\n\n  Time% |     Time |     Imb. |  Imb. | Calls | Group\n...\n||  47.5% | 2.888249 | 0.000057 |  0.0% |   1.0 | #1.selector_function\n||==================================================================\n...\n</code></pre></p> <p>Note</p> <p>Please be cautious that <code>lgp_barrier()</code> will affect the load blance result when analysing with CrayPat region API. Therefore, it is recommended to insert the region before the <code>lgp_barrier()</code> function.</p>"},{"location":"tools/craypat/#collecting-hardware-performance-counters-hwpc","title":"Collecting hardware performance counters (HWPC)","text":"<p>The Performance Application Programming Interface (PAPI) allows you to programmatically collect hardware performance counters (HWPC) in your code. While the user is supposed to manually insert PAPI routines to specify what HWPCs are measured and when to start/stop measuing them, CrayPat dramatically facilitate that process. Specifically, all the user has to do is to just specify HWPC name(s) in an environment variable. Here are the steps to collect HWPCs with CrayPat:</p>"},{"location":"tools/craypat/#step-1-find-available-hardware-counters","title":"Step 1: Find available hardware counters","text":"<p>Available hardware counters can be find with <code>papi_avail</code>.</p> <p>Available hardware counters on Perlmutter <pre><code>PAPI_L1_DCM     Level 1 data cache misses\nPAPI_L2_DCM     Level 2 data cache misses\nPAPI_L2_ICM     Level 2 instruction cache misses\nPAPI_TLB_DM     Data translation lookaside buffer misses\nPAPI_TLB_IM     Instruction translation lookaside buffer misses\nPAPI_BR_MSP     Conditional branch instructions mispredicted\nPAPI_TOT_INS      Instructions completed\nPAPI_FP_INS     Floating point instructions\nPAPI_BR_INS     Branch instructions\nPAPI_VEC_INS      Vector/SIMD instructions (could include integer)\nPAPI_TOT_CYC      Total cycles\nPAPI_L2_DCH     Level 2 data cache hits\nPAPI_L1_DCA     Level 1 data cache accesses\nPAPI_L2_DCR     Level 2 data cache reads\nPAPI_L2_ICH     Level 2 instruction cache hits\nPAPI_L2_ICA     Level 2 instruction cache accesses\nPAPI_L2_ICR     Level 2 instruction cache reads\nPAPI_FML_INS      Floating point multiply instructions\nPAPI_FAD_INS      Floating point add instructions\nPAPI_FDV_INS      Floating point divide instructions\nPAPI_FSQ_INS      Floating point square root instructions\nPAPI_FP_OPS     Floating point operations\n</code></pre></p>"},{"location":"tools/craypat/#step-2-selecting-the-hardware-counters","title":"Step 2: Selecting the hardware counters","text":"<p>Setting the environment variable <code>PAT_RT_PERFCTR</code> to specific events/group:</p> <ul> <li>Predefined Counter Groups, e.g., <code>export PAT_RT_PERFCTR=0</code>.</li> <li>Specify individual events (maximum of 4 event at a time), e.g. <code>export PAT_RT_PERFCTR=\"PAPI_L2_DCM,PAPI_L2_ICM,PAPI_L2_DCH,PAPI_L2_DCR\"</code>.</li> </ul> <p>Note</p> <p>Predefined Counter Groups can be found on slides 38 of NERSC PerformanceTools</p>"},{"location":"tools/craypat/#step-3-run-the-instrumented-excutable-with-craypat","title":"Step 3:  Run the instrumented excutable with CrayPat","text":"<p>After exporting the hardware counter variable we just need to run the instrumented excutable and generate human-readable content with <code>pat_report</code>. A summary for the trace of ROG can be found in the text output of <code>pat_report</code></p> <p>Exemplar output <pre><code>...\nTable 3:  Profile by Function Group and Function\nGroup / Function / PE=HIDE\n==============================================================================\n  Total\n------------------------------------------------------------------------------\n  Time%                                  100.0% \n  Time                                 6.079316 secs\n  Imb. Time                                  -- secs\n  Imb. Time%                                 -- \n  Calls                  66.455 /sec      404.0 calls\n  PAPI_L2_DCM             0.062M/sec    378,368 misses\n  PAPI_L2_ICM             0.051M/sec    310,119 misses\n  PAPI_L2_DCH             0.001G/sec  6,453,273 hits\n  PAPI_L2_DCR             0.001G/sec  6,929,521 ops\n  Average Time per Call                0.015048 secs\n  CrayPat Overhead : Time  0.0%\n  ...\n</code></pre></p>"},{"location":"tools/craypat/#further-readings","title":"Further Readings","text":"<ul> <li>NESRC CrayPat documentation</li> <li>NERSC prepared detailed tutorial on Cray's perftools</li> <li>Cray XC Series Application Programming and Optimization</li> <li>NERSC PerformanceTools</li> </ul>"},{"location":"tools/hclib/","title":"Hclib","text":""},{"location":"tools/hclib/#hclibs-statistic-feature","title":"HClib's statistic feature","text":"<p>In order to take more detailed look about task scheduling, we can enable HClib's statistic feature. It will allow use to trace each worker's statistcs and give their task performance overview including work stealing.</p> <p>Warning</p> <p>Please notice that when enabling the statistic feature, some overhead will be added, so do not use this feature with performance sensitive tasks</p> <p>To enable statistic feature, we need to reinstall the HClib with <code>--enable-stats</code>  option <pre><code>cd $HOME/hclib\n./clobber.sh\n./clean.sh\ngit fetch &amp;&amp; git checkout bale3_actor\n./install.sh --enable-production --enable-stats\nsource hclib-install/bin/hclib_setup_env.sh\n</code></pre></p>"},{"location":"tools/hclib/#example-stats-output","title":"Example stats output","text":"<pre><code>===== HClib statistics: =====  \nWorker 0: 4 tasks executed, 5 tasks spawned, 4 tasks scheduled, 0 steals, 0 stolen tasks, -nan tasks per steal, stolen from = [ 0 ]  \nTotal: 4 tasks, 2 end finishes, 0 future waits, 0 non-blocking end finishes, 1 ctx creates, 2 yields, 1.000000 iters per yield on average\n</code></pre>"},{"location":"tools/slurm/","title":"Slurm","text":""},{"location":"tools/slurm/#useful-tips-when-using-srun","title":"Useful tips when using srun","text":""},{"location":"tools/slurm/#cpu-bind-and-c","title":"<code>cpu-bind</code> and <code>-c</code>","text":"<p><code>cpu-bind=&lt;binding option&gt;</code></p> <ul> <li><code>sockets</code> bind to sockets</li> <li><code>cores</code>bind to cores</li> <li><code>threads</code> bind to threads</li> </ul> <p>When specifying cpu-bind=cores, each PE will be bind to a physical core</p> <p><code>-c &lt;number of logical cores&gt;</code> option sets the \"number of logical cores (CPUs) per task\" for the executable, it is optional for jobs that use one task per physical core.</p> <p>Note</p> <p>On Perlmutter, each CPU-only compute node contains 2 sockets with a total of 128 physical cores where each core has 2 hardware threads, so there are 256 logical CPUs total. For more details, please refer to Perlmutter Architecture</p>"},{"location":"tools/slurm/#examples-at-different-scenarios","title":"Examples at different scenarios","text":"<p>Here we used NERSC Prebuilt Binary <code>check-mpi.cray.pm</code>  to show output at different scenarios</p>"},{"location":"tools/slurm/#scenario-1","title":"scenario  1","text":"<p>Witout specifying both <code>cpu-bind=cores</code> and <code>-c</code> with 4 PEs:</p> <p><code>srun -n 4 check-mpi.cray.pm</code></p> <p>As a result, each PE will spread across different physical cores. <pre><code>Hello from rank 1, on nid004547. (core affinity = 0-255)\nHello from rank 3, on nid004547. (core affinity = 0-255)\nHello from rank 0, on nid004547. (core affinity = 0-255)\nHello from rank 2, on nid004547. (core affinity = 0-255)\n</code></pre></p>"},{"location":"tools/slurm/#scenario-2","title":"scenario  2","text":"<p>When specifying <code>cpu-bind=cores</code> without setting <code>-c</code> with 4 PEs:</p> <p><code>srun -n 4 --cpu-bind=cores check-mpi.cray.pm</code></p> <p>As a result, each PE will be bind to one physical core while each PE may run on different logical core within the same physical core <pre><code>Hello from rank 1, on nid004547. (core affinity = 64,192)\nHello from rank 3, on nid004547. (core affinity = 65,193)\nHello from rank 0, on nid004547. (core affinity = 0,128)\nHello from rank 2, on nid004547. (core affinity = 1,129)\n</code></pre></p> <p>Note</p> <p>On Perlmutter logical core #n and and #n+128 is on the same physical core, e.g. <code>core affinity = 0,128</code> means logical core #0 and #128 and they belong to the same physical core #0</p>"},{"location":"tools/slurm/#scenario-3","title":"scenario  3","text":"<p>When specifying <code>-c 1</code> without setting <code>cpu-bind=cores</code> with 4 PEs:</p> <p><code>srun -n 4 -c 1 check-mpi.cray.pm</code> As a result, each PE will run on one logical core, but two different PE may run within one physical core <pre><code>Hello from rank 0, on nid004547. (core affinity = 0)\nHello from rank 1, on nid004547. (core affinity = 128)\nHello from rank 3, on nid004547. (core affinity = 129)\nHello from rank 2, on nid004547. (core affinity = 1)\n</code></pre> When specifying <code>-c 2</code> without setting <code>cpu-bind=cores</code> with 4 PEs:</p> <p><code>srun -n 4 -c 2 check-mpi.cray.pm</code> As a result, each PE will run on two logical core within one physical core <pre><code>Hello from rank 1, on nid004547. (core affinity = 16,144)\nHello from rank 3, on nid004547. (core affinity = 48,176)\nHello from rank 0, on nid004547. (core affinity = 0,128)\nHello from rank 2, on nid004547. (core affinity = 32,160)\n</code></pre></p>"},{"location":"tools/slurm/#scenario-4","title":"scenario  4","text":"<p>When setting <code>cpu-bind=cores</code> and  <code>-c 1</code> with 4 PEs:</p> <p><code>srun -n 4 -c 1 --cpu-bind=cores check-mpi.cray.pm</code> As a result, each PE will run on two logical core, but two different PE may run within one physical core <pre><code>Hello from rank 0, on nid004547. (core affinity = 0,128)\nHello from rank 1, on nid004547. (core affinity = 1,129)\nHello from rank 3, on nid004547. (core affinity = 1,129)\nHello from rank 2, on nid004547. (core affinity = 0,128)\n</code></pre></p> <p>When setting <code>cpu-bind=cores</code> and  <code>-c 2</code> option:</p> <p><code>srun -n 4 -c 2 --cpu-bind=cores check-mpi.cray.pm</code> Same with scenario  2, each PE will be bind to one physical core while each PE may run on different logical core within the same physical core <pre><code>Hello from rank 1, on nid004547. (core affinity = 16,144)\nHello from rank 0, on nid004547. (core affinity = 0,128)\nHello from rank 3, on nid004547. (core affinity = 48,176)\nHello from rank 2, on nid004547. (core affinity = 32,160)\n</code></pre></p> <p>To make sure our goal here is to make sure each PE bind to one physical core and each socket has an equal number of PEs, we should make sure that we set  <code>cpu-bind=cores</code> and optionally set <code>-c 2</code></p>"},{"location":"tools/tcomm/","title":"HClib T_COMM profiling","text":"<p>This document gives a brief guidance on how to generate HClib T_COMM profiling for actor applications on COTS (typically x86) systems.</p>"},{"location":"tools/tcomm/#step-by-step-guide","title":"Step-by-Step Guide","text":"<p>Here we will take Triangle Counting selector version as an example.</p>"},{"location":"tools/tcomm/#step-1-build-hclib","title":"Step 1: Build HClib","text":"<p>Build the Hclib and setup the environment, please refer to hclib-actor setup page.</p>"},{"location":"tools/tcomm/#step-2-enable-the-tcom-function","title":"Step 2: Enable the TCOM function","text":"<p>Enable the trace function with the macros <code>ENABLE_TCOMM_PROFILING</code> in triangle_selector. User need to put <code>#define ENABLE_TCOMM_PROFILING</code> BEFORE the <code>#include  \"selector.h\"</code> header file to enable the trace function. <pre><code>#include  &lt;math.h&gt;\n#include  &lt;shmem.h&gt;\n...\n#define ENABLE_TCOMM_PROFILING\n#include  \"selector.h\"\n...\n</code></pre> Cautious: Experimental function, user can not define Trace and TCOM at the same time.</p>"},{"location":"tools/tcomm/#step-3-call-the-output-function","title":"Step 3: Call the output function","text":"<p>User need to call the output function <code>print_profiling(char *prefix=\"\")</code>  to print the results, <code>prefix</code> is the label specified as the argument to <code>print_profiling</code> function. </p> <p>For example in TC: <pre><code>...\n#ifdef ENABLE_TCOMM_PROFILING \n    triSelector-&gt;print_profiling(\"tc\");\n#endif\n...\n</code></pre></p>"},{"location":"tools/tcomm/#step-4-recomplie-the-application","title":"Step 4: Recomplie the application","text":"<p>Add PAPI-related options to the <code>Makefile</code> located in <code>$HCLIB_ROOT/../modules/bale_actor/test</code>, e.g., for PACE cluster:</p> <pre><code>PAPI_ROOT=/usr/local/pace-apps/manual/packages/papi/7.0.1/usr/local\n...\n%: %.cpp\n        $(CXX) -g -O3 -std=c++11 -DUSE_SHMEM=1 $(HCLIB_CFLAGS) $(HCLIB_LDFLAGS) -o $@ $^ $(HCLIB_LDLIBS) -I${PAPI_ROOT}/include -L${PAPI_ROOT}/lib -lpapi -lspmat -lconvey -lexstack -llibgetput -lhclib_bale_actor -lm\n</code></pre> <p>Recompile the <code>triangle_selector.cpp</code> application located in <code>$HCLIB_ROOT/../modules/bale_actor/test</code></p> <pre><code>rm triangle_selector\nmake triangle_selector\n</code></pre>"},{"location":"tools/tcomm/#step-5-run-the-application","title":"Step 5: Run the application","text":"<p>Sbatch script example <pre><code>#!/bin/bash\n#SBATCH -q regular\n#SBATCH -N 1\n#SBATCH -n 2 --ntasks-per-node=2\n#SBATCH -C cpu\n#SBATCH -t 00:05:00\n#SBATCH -ooshmem_%j_TC_trace.out\n\nsource  ./oshmem-perlmutter.sh\ncd  $HCLIB_ROOT/../modules/bale_actor/test\necho  \"--------------------------------------------\"\nsrun  --cpu-bind=cores  ./triangle_selector  -f  small.mtx\necho  \"--------------------------------------------\"\n</code></pre></p> <p>Output <pre><code>Running triangle on 8 threads\nModel mask (M) = 15 (should be 1,2,4,8,16 for agi, exstack, exstack2, conveyors, alternates\nalgorithm (a) = 0 (0 for L &amp; L*U, 1 for L &amp; U*L)\nReading file ./small.mtx...\nA has 65536 rows/cols and 909917 nonzeros.\nL has 65536 rows/cols and 909917 nonzeros.\nRun triangle counting ...\nCalculated: Pulls = 148076145\n            Pushes = 99980803\n\nRunning Selector: \n     8.797 seconds:         15673768 triangles\ntc [PE6] T_inside_finish (start - done): 697240862 cycles\ntc [PE6] T_wait (done - finish): 0 cycles\ntc [PE6] T_outside_finish (start - finish): 0 cycles\ntc [PE6] T_sends (count=1317585): 657900662 cycles\ntc [PE6] T_send_in_process (count=0): 0 cycles\ntc [PE6] T_process (count=191674): 19571194 cycles\ntc [PE6] T_TOTAL would be much bigger/smaller than T_finish\ntc [PE6] TCOMM_PROFILING (T_MAIN, T_COMM, T_PROC, T_TOTAL), 39340200, 638329468, 19571194, 697240862\ntc [PE6] TCOMM_PROFILING (T_MAIN/T_TOTAL, T_COMM/T_TOTAL, T_PROC/T_TOTAL), 0.056423, 0.915508, 0.028069\ntc [PE2] T_inside_finish (start - done): 8426154822 cycles\ntc [PE2] T_wait (done - finish): 0 cycles\ntc [PE2] T_outside_finish (start - finish): 0 cycles\n...\ntc [PE5] T_inside_finish (start - done): 658667996 cycles\ntc [PE5] T_wait (done - finish): 0 cycles\ntc [PE5] T_outside_finish (start - finish): 0 cycles\ntc [PE5] T_sends (count=1221575): 621838414 cycles\ntc [PE5] T_send_in_process (count=0): 0 cycles\ntc [PE5] T_process (count=181624): 18025404 cycles\ntc [PE5] T_TOTAL would be much bigger/smaller than T_finish\ntc [PE5] TCOMM_PROFILING (T_MAIN, T_COMM, T_PROC, T_TOTAL), 36829582, 603813010, 18025404, 658667996\ntc [PE5] TCOMM_PROFILING (T_MAIN/T_TOTAL, T_COMM/T_TOTAL, T_PROC/T_TOTAL), 0.055915, 0.916718, 0.027366\n</code></pre> The output generate result for All PEs. </p> <p>Please note that for each PEs: - TCOMM_PROFILING (T_MAIN, T_COMM, T_PROC, T_TOTAL) shows cycles in terms of <code>rdtsc</code> [here]. - TCOMM_PROFILING (T_MAIN/T_TOTAL, T_COMM/T_TOTAL, T_PROC/T_TOTAL) shows the percentage.</p> <p>This profilling function is still experimental, therefore it has the following  limitations/caveats: -   The function can profile a Selector with multiple mailboxes, but the profiler assumes that the main part only does send to MB0, not MB1, MB2, \u2026 -   Use this function just for getting rough information on what part can be a bottleneck.</p>"},{"location":"tools/tcomm/#more-application-exemplar-results","title":"More application exemplar results","text":""},{"location":"tools/tcomm/#histogram","title":"Histogram","text":"<p>Sbatch script example <pre><code>#!/bin/bash\n#SBATCH -q regular\n#SBATCH -N 1\n#SBATCH -n 2 --ntasks-per-node=2\n#SBATCH -C cpu\n#SBATCH -t 00:05:00\n#SBATCH -ooshmem_%j_histo_trace.out\n\nsource  ./perlmutter_setup.sh\ncd  $HCLIB_ROOT/../modules/bale_actor/test\necho  \"--------------------------------------------\"\nsrun  --cpu-bind=cores  ./histo_selector\necho  \"--------------------------------------------\"\n</code></pre></p> <p>Output <pre><code>Running histo on 2 threads\nbuf_cnt (number of buffer pkgs)      (-b)= 1024\nNumber updates / thread              (-n)= 1000000\nTable size / thread                  (-T)= 1000\nmodels_mask                          (-M)= 0\n     0.055 seconds\nhisto [PE0] T_inside_finish (start - done): 134534228 cycles\nhisto [PE0] T_wait (done - finish): 200900 cycles\nhisto [PE0] T_outside_finish (start - finish): 134735128 cycles\nhisto [PE0] T_sends (count=1000000): 108622566 cycles\nhisto [PE0] T_send_in_process (count=0): 0 cycles\nhisto [PE0] T_process (count=1000129): 25648406 cycles\nhisto [PE0] TCOMM_PROFILING (T_MAIN, T_COMM, T_PROC, T_TOTAL), 25911662, 83175060, 25648406, 134735128\nhisto [PE0] TCOMM_PROFILING (T_MAIN/T_TOTAL, T_COMM/T_TOTAL, T_PROC/T_TOTAL), 0.192316, 0.617323, 0.190362\nhisto [PE1] T_inside_finish (start - done): 133603939 cycles\nhisto [PE1] T_wait (done - finish): 1096963 cycles\nhisto [PE1] T_outside_finish (start - finish): 134700902 cycles\nhisto [PE1] T_sends (count=1000000): 107848089 cycles\nhisto [PE1] T_send_in_process (count=0): 0 cycles\nhisto [PE1] T_process (count=999871): 25657456 cycles\nhisto [PE1] TCOMM_PROFILING (T_MAIN, T_COMM, T_PROC, T_TOTAL), 25755850, 83287596, 25657456, 134700902\nhisto [PE1] TCOMM_PROFILING (T_MAIN/T_TOTAL, T_COMM/T_TOTAL, T_PROC/T_TOTAL), 0.191208, 0.618315, 0.190477\n</code></pre></p>"},{"location":"tools/tcomm/#index-gather","title":"Index Gather","text":"<p>Sbatch script example <pre><code>#!/bin/bash\n#SBATCH -q regular\n#SBATCH -N 1\n#SBATCH -n 2 --ntasks-per-node=2\n#SBATCH -C cpu\n#SBATCH -t 00:05:00\n#SBATCH -ooshmem_%j_ig_trace.out\n\nsource  ./perlmutter_setup.sh\ncd  $HCLIB_ROOT/../modules/bale_actor/test\necho  \"--------------------------------------------\"\nsrun  --cpu-bind=cores  ./ig_selector\necho  \"--------------------------------------------\"\n</code></pre></p> <p>Output <pre><code>Running ig on 2 threads\nbuf_cnt (number of buffer pkgs)      (-b)= 1024\nNumber of Request / thread           (-n)= 1000000\nTable size / thread                  (-T)= 100000\nmodels_mask                          (-M)= 0\nmodels_mask is or of 1,2,4,8,16 for agi,exstack,exstack2,conveyor,alternate)\n     0.120 seconds\nig [PE1] T_inside_finish (start - done): 291840251 cycles\nig [PE1] T_wait (done - finish): 437570 cycles\nig [PE1] T_outside_finish (start - finish): 292277821 cycles\nig [PE1] T_sends (count=1000000): 265550829 cycles\nig [PE1] T_send_in_process (count=1000665): 109491048 cycles\nig [PE1] T_process (count=2000665): 193290221 cycles\nig [PE1] TCOMM_PROFILING (T_MAIN, T_COMM, T_PROC, T_TOTAL), 26289422, 182189226, 83799173, 292277821\nig [PE1] TCOMM_PROFILING (T_MAIN/T_TOTAL, T_COMM/T_TOTAL, T_PROC/T_TOTAL), 0.089947, 0.623343, 0.286711\nig [PE0] T_inside_finish (start - done): 284089186 cycles\nig [PE0] T_wait (done - finish): 8140370 cycles\nig [PE0] T_outside_finish (start - finish): 292229556 cycles\nig [PE0] T_sends (count=1000000): 257481362 cycles\nig [PE0] T_send_in_process (count=999335): 107141854 cycles\nig [PE0] T_process (count=1999335): 186535650 cycles\nig [PE0] TCOMM_PROFILING (T_MAIN, T_COMM, T_PROC, T_TOTAL), 26607824, 186227936, 79393796, 292229556\nig [PE0] TCOMM_PROFILING (T_MAIN/T_TOTAL, T_COMM/T_TOTAL, T_PROC/T_TOTAL), 0.091051, 0.637266, 0.271683\n</code></pre></p> <p>More example can also be find within the test directory.</p>"},{"location":"tools/trace/","title":"HClib Network Trace Generation","text":"<p>This document gives a brief guidance on how to generate trace for actor applications.</p>"},{"location":"tools/trace/#step-by-step-guide","title":"Step-by-Step Guide","text":"<p>Here we will take Triangle Counting selector version as an example.</p>"},{"location":"tools/trace/#step-1-build-hclib","title":"Step 1: Build HClib","text":"<p>Build the Hclib and setup the environment, please refer to hclib-actor setup page.</p>"},{"location":"tools/trace/#step-2-enable-the-trace-function","title":"Step 2: Enable the Trace function","text":"<p>Enable the trace function with the macros <code>ENABLE_TRACE</code> in triangle_selector. User need to put <code>#define ENABLE_TRACE</code> BEFORE the <code>#include  \"selector.h\"</code> header file to enable the trace function. <pre><code>#include  &lt;math.h&gt;\n#include  &lt;shmem.h&gt;\n...\n#define ENABLE_TRACE\n#include  \"selector.h\"\n...\n</code></pre> Cautious: Experimental function, user may encounter redefinition issue.</p>"},{"location":"tools/trace/#step-3-recomplie-the-application","title":"Step 3: Recomplie the application","text":"<p>Recompile the <code>triangle_selector.cpp</code> application located in <code>$HCLIB_ROOT/../modules/bale_actor/test</code></p> <pre><code>rm triangle_selector\nmake triangle_selector\n</code></pre>"},{"location":"tools/trace/#step-4-run-the-application","title":"Step 4: Run the application","text":"<p>Sbatch script example <pre><code>#!/bin/bash\n#SBATCH -q regular\n#SBATCH -N 1\n#SBATCH -n 2 --ntasks-per-node=2\n#SBATCH -C cpu\n#SBATCH -t 00:05:00\n#SBATCH -ooshmem_%j_TC_trace.out\n\nsource  ./perlmutter_setup.sh\ncd  $HCLIB_ROOT/../modules/bale_actor/test\necho  \"--------------------------------------------\"\nsrun  --cpu-bind=cores  ./triangle_selector  -f  small.mtx\necho  \"--------------------------------------------\"\n</code></pre></p> <p>Output <pre><code>Running triangle on 2 threads\nModel mask (M) = 15 (should be 1,2,4,8,16 for agi, exstack, exstack2, conveyors, alternates\nalgorithm (a) = 0 (0 for L &amp; L*U, 1 for L &amp; U*L)\nReading file small.mtx...\nA has 65536 rows/cols and 909917 nonzeros.\nL has 65536 rows/cols and 909917 nonzeros.\nRun triangle counting ...\nCalculated: Pulls = 148076145\n            Pushes = 99980803\n\nRunning Selector: \n   113.592 seconds:         15673768 triangles\nLogical actor message trace enabled\nPE:0, Node 0\nPE:1, Node 0\n</code></pre> It will also generate data files for each PE. In this example, two data file <code>PE0_send.dat</code> and <code>PE1_send.dat</code> were generated since we ran this application on two thread.</p> <p>The Format of the data is show as below:</p> <p>SourceID (node,PE), DestID (node,pe), pkt size,  <p>E.g. 3, 15, 1, 5, 16, 1689905738.916986</p>"},{"location":"tools/trace/#more-application-exemplar-results","title":"More application exemplar results","text":""},{"location":"tools/trace/#histogram","title":"Histogram","text":"<p>Sbatch script example <pre><code>#!/bin/bash\n#SBATCH -q regular\n#SBATCH -N 1\n#SBATCH -n 2 --ntasks-per-node=2\n#SBATCH -C cpu\n#SBATCH -t 00:05:00\n#SBATCH -ooshmem_%j_histo_trace.out\n\nsource  ./perlmutter_setup.sh\ncd  $HCLIB_ROOT/../modules/bale_actor/test\necho  \"--------------------------------------------\"\nsrun  --cpu-bind=cores  ./histo_selector\necho  \"--------------------------------------------\"\n</code></pre></p> <p>Output <pre><code>Running histo on 2 threads\nbuf_cnt (number of buffer pkgs)      (-b)= 1024\nNumber updates / thread              (-n)= 1000000\nTable size / thread                  (-T)= 1000\nmodels_mask                          (-M)= 0\n    19.743 seconds\nLogical actor message trace enabled\nPE:0, Node 0\nPE:1, Node 1\n</code></pre></p>"},{"location":"tools/trace/#index-gather","title":"Index Gather","text":"<p>Sbatch script example <pre><code>#!/bin/bash\n#SBATCH -q regular\n#SBATCH -N 1\n#SBATCH -n 2 --ntasks-per-node=2\n#SBATCH -C cpu\n#SBATCH -t 00:05:00\n#SBATCH -ooshmem_%j_ig_trace.out\n\nsource  ./perlmutter_setup.sh\ncd  $HCLIB_ROOT/../modules/bale_actor/test\necho  \"--------------------------------------------\"\nsrun  --cpu-bind=cores  ./ig_selector\necho  \"--------------------------------------------\"\n</code></pre></p> <p>Output <pre><code>Running ig on 2 threads\nbuf_cnt (number of buffer pkgs)      (-b)= 1024\nNumber of Request / thread           (-n)= 1000000\nTable size / thread                  (-T)= 100000\nmodels_mask                          (-M)= 0\nmodels_mask is or of 1,2,4,8,16 for agi,exstack,exstack2,conveyor,alternate)\n    39.603 seconds\nLogical actor message trace enabled\nPE:0, Node 0\nPE:1, Node 1\n</code></pre></p> <p>More example can also be find within the test directory.</p>"}]}